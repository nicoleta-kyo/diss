{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Update RBM and ESN in order to implement them working together\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Code for ising taken from Aguilera's code for \"Adaptation to Criticality\":\n",
    "Modifications:\n",
    "- Restricted - connections within hidden layer not being updated\n",
    "- Embodiment for environment Frozen Lake\n",
    "- RL Learning by a variant of SARSA - to Hinton's paper \"Using Q-energies\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import math\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class ising:\n",
    "    # Initialize the network\n",
    "    def __init__(self, netsize, Nsensors=1, Nmotors=1):  # Create ising model\n",
    "\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        self.size = netsize\t\t#Network size\n",
    "        self.Ssize = Nsensors  # Number of sensors\n",
    "        self.Msize = Nmotors  # Number of motors\n",
    "\n",
    "        self.h = np.zeros(netsize) # local biases\n",
    "        self.J = np.zeros((netsize, netsize)) # symmetic weights between hidden variables\n",
    "        self.max_weights = 2          # why do we need to restrict the weights to a maximum value?\n",
    "\n",
    "        self.randomize_state()\n",
    "\n",
    "        self.env = gym.make('FrozenLake8x8-v0')\n",
    "        self.observation = self.env.reset()\n",
    "\n",
    "        self.BetaCrit = 1.0  \n",
    "        self.defaultT = max(100, netsize * 20)\n",
    "        \n",
    "        self.defaultGamma = 0.999  # as in Otsuka - Solving POMPDs\n",
    "        self.defaultLr = 0.01  # as in Otsuka - Solving POMPDs\n",
    "\n",
    "        self.Ssize1 = 0\n",
    "        \n",
    "        self.rewardsPerEpisode = 0     #keep track of rewards\n",
    "        self.successfulEpisodes = 0\n",
    "        self.observations = np.repeat(-1,1000*5000)    #keep track of reached states\n",
    "      \n",
    "    def UpdateMemoryNodes(self):\n",
    "        \n",
    "      \n",
    "    def initialise_wiring(self):\n",
    "        self.J = np.random.normal(0, 0.1, (self.size, self.size))\n",
    "\n",
    "    def get_state(self, mode='all'):\n",
    "        if mode == 'all':\n",
    "            return self.s\n",
    "        elif mode == 'motors':\n",
    "            return self.motors\n",
    "        elif mode == 'sensors':          # isn't mode sensors the same as input??\n",
    "            return self.s[0:self.Ssize]\n",
    "        elif mode == 'input':\n",
    "            return self.sensors\n",
    "        elif mode == 'non-sensors':\n",
    "            return self.s[self.Ssize:]\n",
    "        elif mode == 'hidden':\n",
    "            return self.s[self.Ssize:-self.Msize]\n",
    "\n",
    "    # gets the index of the configuration of the neurons - state as one number\n",
    "    def get_state_index(self, mode='all'):\n",
    "        return bool2int(0.5 * (self.get_state(mode) + 1))\n",
    "\n",
    "    # Randomize the state of the network\n",
    "    def randomize_state(self):\n",
    "        self.s = np.random.randint(0, 2, self.size) * 2 - 1           # make units -1 or 1 \n",
    "        self.sensors = np.random.randint(0, 2, self.Ssize) * 2 - 1    # make sensors -1 or 1\n",
    "        self.motors = np.random.randint(0, 2, self.Msize) * 2 - 1 \n",
    "\n",
    "    # Randomize the position of the agent\n",
    "    def randomize_position(self):\n",
    "        self.observation = self.env.reset()\n",
    "\n",
    "    # Set random bias to sets of units of the system\n",
    "    # bias to hidden and action\n",
    "    def random_fields(self, max_weights=None):\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        self.h[self.Ssize:] = max_weights * \\\n",
    "            (np.random.rand(self.size - self.Ssize) * 2 - 1)\n",
    "\n",
    "    # Set random connections to sets of units of the system\n",
    "    def random_wiring(self, max_weights=None):  # Set random values for J\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        for i in range(self.size):\n",
    "            for j in np.arange(i + 1, self.size):\n",
    "                if i < j and (i >= self.Ssize or j >= self.Ssize):  # don't add connections between the sensors\n",
    "                    # what about the motors? connection between the two will be added: doesn't matter because the correlations are not considered?\n",
    "                    self.J[i, j] = (np.random.rand(1) * 2 - 1) * self.max_weights\n",
    "        # get rid of connections between hidden units, and also motor (still dont know why this hasnt been done)\n",
    "        self.J[self.Ssize:-self.Msize,self.Ssize:-self.Msize] = 0  # between hidden\n",
    "        self.J[:self.Ssize,-self.Msize:] = 0             # between sensor and motor\n",
    "        self.J[-self.Msize:,-self.Msize:] = 0            # between motor\n",
    "\n",
    "    # Update the position of the agent\n",
    "    def Move(self):\n",
    "        action = int(np.digitize(\n",
    "            np.sum(self.s[-self.Msize:]) / self.Msize, [-2, -1/2, 0, 1/2, 2])) - 1\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        \n",
    "        self.rewardsPerEpisode += reward    #update rewards per episode\n",
    "        self.observations[(self.observations == -1).argmax()] = observation      #add to list woth visited states\n",
    "\n",
    "    # Update the state of the sensor\n",
    "    def UpdateSensors(self, state=None):\n",
    "        if state is None:\n",
    "            self.sensors = 2 * bitfield(self.observation, self.Ssize) - 1\n",
    "        else:\n",
    "            self.sensors = 2 * bitfield(state, self.Ssize) - 1    \n",
    "        \n",
    "    \n",
    "    # Update the state of the motor?\n",
    "    def UpdateMotors(self, action):\n",
    "        self.motors = 2 * bitfield(action, self.Msize) - 1\n",
    "\n",
    "    # Execute step of the Glauber algorithm to update the state of one unit\n",
    "    def GlauberStep(self, i=None): \n",
    "        if i is None:\n",
    "            i = np.random.randint(self.size)\n",
    "\n",
    "        I = 0\n",
    "        if i < self.Ssize:\n",
    "            I = self.sensors[i]\n",
    "        eDiff = 2 * self.s[i] * (self.h[i] + I +\n",
    "                                 np.dot(self.J[i, :] + self.J[:, i], self.s))\n",
    "        if eDiff * self.BetaCrit < np.log(1 / np.random.rand() - 1):    # Glauber\n",
    "            self.s[i] = -self.s[i]\n",
    "\n",
    "    # Update random unit of the agent\n",
    "    def Update(self, i=None):\n",
    "        if i is None:\n",
    "            i = np.random.randint(-1, self.size)\n",
    "        if i == -1:\n",
    "            self.Move()\n",
    "            self.UpdateSensors()\n",
    "        else:\n",
    "            self.GlauberStep(i)\n",
    "\n",
    "    # Sequentially update state of all units of the agent in random order\n",
    "    def SequentialUpdate(self):\n",
    "        for i in np.random.permutation(range(-1, self.size)):\n",
    "            self.Update(i)\n",
    "\n",
    "    # Step of the learning algorith to ajust correlations to the critical regime\n",
    "    def AdjustCorrelations(self, T=None):\n",
    "        if T is None:\n",
    "            T = self.defaultT\n",
    "\n",
    "        self.m = np.zeros(self.size)\n",
    "        self.c = np.zeros((self.size, self.size))\n",
    "        self.C = np.zeros((self.size, self.size))\n",
    "\n",
    "        # Main simulation loop:\n",
    "        self.x = np.zeros(T)      # to store the positions of the car during all T\n",
    "        samples = []\n",
    "        for t in range(T):\n",
    "\n",
    "            self.SequentialUpdate()\n",
    "            self.x[t] = self.observation\n",
    "            self.m += self.s\n",
    "            for i in range(self.size):\n",
    "                self.c[i, i + 1:] += self.s[i] * self.s[i + 1:]\n",
    "        self.m /= T\n",
    "        self.c /= T\n",
    "        for i in range(self.size):\n",
    "            self.C[i, i + 1:] = self.c[i, i + 1:] - self.m[i] * self.m[i + 1:]\n",
    "\n",
    "        c1 = np.zeros((self.size, self.size))\n",
    "        for i in range(self.size):\n",
    "            inds = np.array([], int)\n",
    "            c = np.array([])\n",
    "            for j in range(self.size):\n",
    "                if not i == j:\n",
    "                    inds = np.append(inds, [j])\n",
    "                if i < j:\n",
    "                    c = np.append(c, [self.c[i, j]])\n",
    "                elif i > j:\n",
    "                    c = np.append(c, [self.c[j, i]])\n",
    "            order = np.argsort(c)[::-1]\n",
    "            c1[i, inds[order]] = self.Cint[i, :]\n",
    "        self.c1 = np.triu(c1 + c1.T, 1)\n",
    "        self.c1 *= 0.5\n",
    "\n",
    "        self.m[0:self.Ssize] = 0          \n",
    "        self.m1[0:self.Ssize] = 0     #sensors have objective mean 0 but in the paper they say it's all of the units but the sensors that have mean 0??\n",
    "        self.c[0:self.Ssize, 0:self.Ssize] = 0    #set corr in between sensors to 0\n",
    "        self.c[-self.Msize:, -self.Msize:] = 0    #set corr in between motors to 0\n",
    "        self.c[0:self.Ssize, -self.Msize:] = 0    #set corr between sensors and motors to 0\n",
    "        self.c1[0:self.Ssize, 0:self.Ssize] = 0\n",
    "        self.c1[-self.Msize:, -self.Msize:] = 0\n",
    "        self.c1[0:self.Ssize, -self.Msize:] = 0\n",
    "        \n",
    "        # make it restricted BM\n",
    "        self.c[self.Ssize:-self.Msize,self.Ssize:-self.Msize] = 0   #set corr in between hidden units to 0\n",
    "        self.c1[self.Ssize:-self.Msize,self.Ssize:-self.Msize] = 0   #for reference as well\n",
    "        \n",
    "        dh = self.m1 - self.m\n",
    "        dJ = self.c1 - self.c\n",
    "\n",
    "        return dh, dJ\n",
    "\n",
    "    # Algorithm for poising an agent in a critical regime\n",
    "    def CriticalLearning(self, Iterations, T=None):\n",
    "        u = 0.01\n",
    "        count = 0\n",
    "        dh, dJ = self.AdjustCorrelations(T)\n",
    "        for it in range(Iterations):\n",
    "            count += 1\n",
    "            self.h += u * dh\n",
    "            self.J += u * dJ\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                self.randomize_state()\n",
    "                self.randomize_position()\n",
    "                \n",
    "                if self.rewardsPerEpisode >= 1:     # keep track of the times the agent reached the goal\n",
    "                    self.successfulEpisodes += 1\n",
    "                self.rewardsPerEpisode = 0\n",
    "                \n",
    "            Vmax = self.max_weights\n",
    "            for i in range(self.size):\n",
    "                if np.abs(self.h[i]) > Vmax:        # why do we need to restrict the weights and biases to a maximum value?\n",
    "                    self.h[i] = Vmax * np.sign(self.h[i])\n",
    "                for j in np.arange(i + 1, self.size):\n",
    "                    if np.abs(self.J[i, j]) > Vmax:\n",
    "                        self.J[i, j] = Vmax * np.sign(self.J[i, j])\n",
    "\n",
    "            dh, dJ = self.AdjustCorrelations(T)\n",
    "           \n",
    "    # Calculate hat_h_k for each k (expert)    \n",
    "    def ExpectedValueExperts(self, sensors, motors):\n",
    "        # self.s[Ssize:-Msize] - all hidden\n",
    "        # self.h[Ssize:-Msize] - biases of hidden\n",
    "        # self.J[:Ssize,Ssize:-Msize] - sensor-hidden connections\n",
    "        # self.J[Ssize:-Msize, -Msize:] - hidden-motor conn.\n",
    "        \n",
    "        num_e = self.size - (self.Ssize+self.Msize)  # number of hidden units (experts)\n",
    "        ve = np.zeros(num_e)             # array of expected values for experts\n",
    "        for k in range(num_e):\n",
    "            ve[k] = sigmoid(np.dot(self.J[0:self.Ssize,self.Ssize:-self.Msize][:,k], sensors) + \n",
    "                            np.dot(self.J[self.Ssize:-self.Msize, -self.Msize:][k,:], motors) +\n",
    "                            self.h[self.Ssize:-self.Msize][k])\n",
    "        \n",
    "        return ve\n",
    "    \n",
    "    # takes in state and action and uses their binary equivalents to calculate the energy (not the actual network's)\n",
    "    def CalcFreeEnergy(self, state, action):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        sensors = 2 * bitfield(state, self.Ssize) - 1  #simulate sensors,motors of the network for the state, action\n",
    "        motors = 2 * bitfield(action, self.Msize) - 1\n",
    "        \n",
    "        ve = self.ExpectedValueExperts(sensors, motors).reshape(-1,1)  #calculate expected hidden values\n",
    "        ss = sensors.reshape(1,-1)\n",
    "        sm = motors.reshape(1,-1) \n",
    "        \n",
    "        # calculate the negative log-likelihood\n",
    "        a = -np.dot(np.dot(ss, self.J[:self.Ssize,self.Ssize:-self.Msize]), ve)    # 1\n",
    "        b = -np.dot(ss, self.h[:self.Ssize].reshape(-1,1))                             # 2\n",
    "        c = -np.dot(np.dot(sm, np.transpose(self.J[self.Ssize:-self.Msize, -self.Msize:])), ve)  # 3\n",
    "        d = -np.dot(sm, self.h[-self.Msize:].reshape(-1,1))                            # 4\n",
    "        e = -np.dot(self.h[self.Ssize:-self.Msize], ve)                       # 5\n",
    "        \n",
    "        # calculate the negative entropy\n",
    "        f = 0                                                           # 6 + 7\n",
    "        for k in range(len(ve)):\n",
    "            f += ve[k]*np.log(ve[k]) + (1 - ve[k])*np.log(1 - ve[k])\n",
    "        \n",
    "        return (a+b+c+d+e+f)\n",
    "    \n",
    "    def SarsaLearning(self, total_episodes, max_steps, Beta, gamma=None, lr=None):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        self.rewards = np.zeros(total_episodes)\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            \n",
    "            beta = Beta[episode]\n",
    "            \n",
    "            state = self.env.reset()\n",
    "            self.UpdateSensors()\n",
    "\n",
    "            action = self.ChooseAction(state, beta)\n",
    "            self.UpdateMotors(action)\n",
    "           \n",
    "            Q1 = -1*self.CalcFreeEnergy(state, action)   #calculate Q1 = Q(s,a) = -F(s,a)\n",
    "\n",
    "            t = 0\n",
    "            while t < max_steps:\n",
    "                \n",
    "#                self.env.render()\n",
    "                \n",
    "                state2, reward, done, info = self.env.step(action)\n",
    "\n",
    "                action2 = self.ChooseAction(state2, beta)\n",
    "                \n",
    "                Q2 = -1*self.CalcFreeEnergy(state2, action2)        #calculate Q2 = Q(s',a') = -F(s',a')\n",
    "\n",
    "                self.SarsaUpdate(Q1, reward, Q2, gamma, lr)\n",
    "\n",
    "                #update the network\n",
    "                self.UpdateSensors(state2)\n",
    "                self.UpdateMotors(action2)\n",
    "                action = action2   # no need to update state, because not needed - Q1 is directly calculated\n",
    "                Q1 = Q2\n",
    "\n",
    "                t += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            self.rewards[episode] = reward\n",
    "    \n",
    "    # works with the network's actual sensors and motors\n",
    "    def SarsaUpdate(self, Q1, reward, Q2, gamma, lr):\n",
    "        \n",
    "        if gamma is None:\n",
    "            gamma = self.defaultGamma\n",
    "        if lr is None:\n",
    "            lr = self.defaultLr\n",
    "        \n",
    "        rDiff = lr*(reward + gamma * Q2 - Q1)\n",
    "        \n",
    "        # calculate dQ/Wsensors\n",
    "        ve = self.ExpectedValueExperts(self.sensors, self.motors).reshape(1,-1)\n",
    "        ve_new = np.repeat(ve, len(self.sensors)).reshape(-1,1)\n",
    "        s_ne = np.repeat(self.sensors.reshape(1,-1), ve.shape[1], axis=0)\n",
    "        s_new = s_ne.reshape(s_ne.shape[0]*s_ne.shape[1],1)\n",
    "        dQdWs = np.multiply(ve_new,s_new)\n",
    "        \n",
    "        # calculate dQ/Wmotors\n",
    "        ve_new = np.repeat(ve, len(self.motors)).reshape(-1,1)\n",
    "        m_ne = np.repeat(self.motors.reshape(1,-1), ve.shape[1], axis=0)\n",
    "        m_new = m_ne.reshape(m_ne.shape[0]*m_ne.shape[1],1)\n",
    "        dQdWm = np.multiply(ve_new,m_new)\n",
    "        \n",
    "        # update weights of sensors and motors\n",
    "        num_h = self.size - (self.Ssize+self.Msize)\n",
    "        \n",
    "        dWs = (rDiff*dQdWs).reshape(num_h, self.Ssize)\n",
    "        dWm = (rDiff*dQdWm).reshape(num_h, self.Msize)\n",
    "        \n",
    "        self.J[:self.Ssize,self.Ssize:-self.Msize] = np.transpose(np.transpose(self.J[:self.Ssize,self.Ssize:-self.Msize]) + dWs)\n",
    "        self.J[self.Ssize:-self.Msize, -self.Msize:] = self.J[self.Ssize:-self.Msize, -self.Msize:] + dWm\n",
    "        \n",
    "        # update biases\n",
    "        #!! not in the paper - I think this should be it??\n",
    "        self.h[:self.Ssize] = self.h[:self.Ssize] + rDiff*self.sensors\n",
    "        self.h[-self.Msize:] = self.h[-self.Msize:] + rDiff*self.motors\n",
    "        self.h[self.Ssize:-self.Msize] =  self.h[self.Ssize:-self.Msize] + rDiff*ve\n",
    "        \n",
    "        \n",
    "    def ChooseAction(self, state, beta):\n",
    "        \n",
    "        try:\n",
    "            # calculate probabilities of all actions - based on Otsuka's paper\n",
    "            p_a = np.zeros((self.env.nA,2))\n",
    "            for a in range(self.env.nA):\n",
    "                fa = np.exp(-beta*self.CalcFreeEnergy(state, a))\n",
    "                fa_ = 0\n",
    "                for a_ in range(self.env.nA):\n",
    "                    fa_ = fa_ + np.exp(-beta*self.CalcFreeEnergy(state, a_))\n",
    "                p_a[a,1] = fa/fa_\n",
    "                p_a[a,0] = a  #add index column\n",
    "\n",
    "            # sample an action from the distribution\n",
    "            ord_p_a = p_a[p_a[:,1].argsort()]\n",
    "            ord_p_a[:,1] = np.cumsum(ord_p_a[:,1])\n",
    "\n",
    "            b = np.array(ord_p_a[:,1] > np.random.rand())\n",
    "            act = int(ord_p_a[b.argmax(),0])  # take the index of the chosen action\n",
    "        \n",
    "        except RuntimeWarning:\n",
    "            pdb.set_trace()\n",
    " \n",
    "        return act\n",
    "        \n",
    "        \n",
    "# Transform bool array into positive integer\n",
    "# [nk] encodes the state of the neurons as one number\n",
    "def bool2int(x):\n",
    "    y = 0\n",
    "    for i, j in enumerate(np.array(x)[::-1]):\n",
    "        y += j * 2**i\n",
    "    return int(y)\n",
    "\n",
    "# Transform positive integer into bit array\n",
    "def bitfield(n, size):\n",
    "    x = [int(x) for x in bin(int(n))[2:]]\n",
    "    x = [0] * (size - len(x)) + x\n",
    "    return np.array(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
