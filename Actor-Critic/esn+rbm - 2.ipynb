{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nesn+rbm - episodic update with the following additions:\\nesn:\\n- bias node for input\\n- bias for the reservoir activations (b = -0.5*W)\\nrbm:\\ncleaned up code\\nrun:\\n-activation functions for the esn: tanh/atanh\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "esn+rbm - episodic update with the following additions:\n",
    "esn:\n",
    "- bias node for input\n",
    "- bias for the reservoir activations (b = -0.5*W)\n",
    "rbm:\n",
    "cleaned up code\n",
    "run:\n",
    "-activation functions for the esn: identity (included version with tanh/atanh\n",
    "but atanh is not defined outside of (-1,1) so i'm not using it for now)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "ESN Implementation imported from: https://github.com/cknd/pyESN/blob/master/pyESN.py\n",
    "\n",
    "Initialisation of reservoir weights and input weights done according to Jaeger's paper,\n",
    "and implementation of augmented training algorithm following the paper:\n",
    "    https://papers.nips.cc/paper/2318-adaptive-nonlinear-system-identification-with-echo-state-networks.pdf  \n",
    "\n",
    "Readout training with Moore-Penrose Matrix Inverse or Ridge Regression (added)\n",
    "\n",
    "Changes for Otsuka's model architecture:\n",
    "    - sigmoid function for update of memory layer (reservoir units)\n",
    "    - ???? initialisation of reservoir weights\n",
    "    - default out activation: tanh\n",
    "    - get_states method to work with continuation\n",
    "    - fit and predict methods take reservoir units (output from get_states) directly\n",
    "Intrinsic Motivation Schmidhuber\n",
    "    - history object: for each n in N_T (episodes*num_steps): [state, action, state2, reward]\n",
    "    - evalHistory and calculateReward methods\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from cannon_rlsfilter import RLSFilterAnalyticIntercept\n",
    "import math\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"error\")\n",
    "\n",
    "\n",
    "\n",
    "def correct_dimensions(s, targetlength):\n",
    "    \"\"\"checks the dimensionality of some numeric argument s, broadcasts it\n",
    "       to the specified length if possible.\n",
    "    Args:\n",
    "        s: None, scalar or 1D array\n",
    "        targetlength: expected length of s\n",
    "    Returns:\n",
    "        None if s is None, else numpy vector of length targetlength\n",
    "    \"\"\"\n",
    "    if s is not None:\n",
    "        s = np.array(s)\n",
    "        if s.ndim == 0:\n",
    "            s = np.array([s] * targetlength)\n",
    "        elif s.ndim == 1:\n",
    "            if not len(s) == targetlength:\n",
    "                raise ValueError(\"arg must have length \" + str(targetlength))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x, gain=1):\n",
    "    return 1 / (1 + np.exp(-gain*x))\n",
    "\n",
    "# !!! do I apply the gain to the predictions as well?\n",
    "def inv_sigmoid(x, gain=1):\n",
    "    return np.log( (x*gain) / (1 - (x*gain) ) )\n",
    "\n",
    "def atanh(x):\n",
    "    #x is of shape (1, teachers)\n",
    "    \n",
    "    atanhx = np.zeros(x.shape[1])\n",
    "    for i,v in enumerate(x[0]):\n",
    "        atanhx[i] = math.atanh(v)\n",
    "        \n",
    "    return atanhx\n",
    "        \n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class ESN():\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, n_reservoir=200,\n",
    "                 spectral_radius=0.95, sparsity=0,\n",
    "                 noise=0.001,\n",
    "                 readout='pseudo-inverse',\n",
    "                 ridge_reg=None,\n",
    "                 input_weights_scaling = 1,\n",
    "                 input_scaling=None,input_shift=None,teacher_forcing=None, feedback_scaling=None,\n",
    "                 teacher_scaling=None, teacher_shift=None,\n",
    "                 out_activation=np.tanh, inverse_out_activation=atanh,\n",
    "                 silent=True, \n",
    "                 augmented=False,\n",
    "                 transient=200,\n",
    "                 input_bias=0\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_inputs: nr of input dimensions\n",
    "            n_outputs: nr of output dimensions\n",
    "            n_reservoir: nr of reservoir neurons\n",
    "            spectral_radius: spectral radius of the recurrent weight matrix\n",
    "            sparsity: proportion of recurrent weights set to zero\n",
    "            noise: noise added to each neuron (regularization)\n",
    "            readout: type of readout 0 can be moonrose pseudo-inverse or ridge regression\n",
    "            ridge_reg: regularisation value alpha if readout is Ridge\n",
    "            \n",
    "            input_weights_scaling: scaling of the input connection weights\n",
    "            input_shift: scalar or vector of length n_inputs to add to each\n",
    "                        input dimension before feeding it to the network.                       \n",
    "            input_scaling: scalar or vector of length n_inputs to multiply\n",
    "                        with each input dimension before feeding it to the netw.\n",
    "                        \n",
    "            teacher_shift: additive term applied to the target signal\n",
    "            teacher_scaling: factor applied to the target signal\n",
    "            teacher_forcing: if True, feed the target back into output units\n",
    "\n",
    "            out_activation: output activation function (applied to the readout)\n",
    "            inverse_out_activation: inverse of the output activation function\n",
    "    \n",
    "            silent: supress messages\n",
    "            augmented: if True, use augmented training algorithm\n",
    "            transient: how many initial states to discard\n",
    "            \n",
    "        \"\"\"\n",
    "        # check for proper dimensionality of all arguments and write them down.\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_reservoir = n_reservoir\n",
    "        self.n_outputs = n_outputs   # part will be obs, part will be reward\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.readout = readout\n",
    "        self.ridge_reg = ridge_reg\n",
    "        \n",
    "        self.input_weights_scaling = input_weights_scaling\n",
    "        self.input_shift = correct_dimensions(input_shift, n_inputs)\n",
    "        self.input_scaling = correct_dimensions(input_scaling, n_inputs)\n",
    "\n",
    "        self.teacher_shift = teacher_shift\n",
    "        self.teacher_scaling = teacher_scaling\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "\n",
    "        self.out_activation = out_activation\n",
    "        self.inverse_out_activation = inverse_out_activation\n",
    "\n",
    "        self.silent = silent\n",
    "        self.augmented = augmented\n",
    "        self.transient = transient\n",
    "        \n",
    "        self.input_bias = input_bias\n",
    "        \n",
    "        self.laststate = np.zeros(self.n_reservoir)\n",
    "        self.lastextendedstate = np.zeros(self.n_reservoir+self.n_inputs)\n",
    "        self.lastinput = np.zeros(self.n_inputs+self.input_bias)\n",
    "        self.lastoutput = np.zeros(self.n_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.defaultHistEval = 10000    # look at last n time steps when evaluating history\n",
    "        \n",
    "        self.initweights()\n",
    "        \n",
    "    def initweights(self):\n",
    "        \n",
    "        # initialize recurrent weights:\n",
    "        self.W = self.initialise_reservoir()\n",
    "        \n",
    "        # bias for the update of the states of the reservoir\n",
    "        self.reservoir_bias = np.dot(np.repeat(-0.5,self.n_reservoir),self.W)\n",
    "            \n",
    "        # [nk] following Jaeger's paper:\n",
    "        # added scaling\n",
    "        self.W_in = np.random.uniform(low = -0.1, high = 0.1, size = (self.n_reservoir, self.n_inputs+self.input_bias))*self.input_weights_scaling\n",
    "             \n",
    "        # random feedback (teacher forcing) weights:\n",
    "        self.W_feedb = np.random.RandomState().rand(\n",
    "            self.n_reservoir, self.n_outputs) * 2 - 1\n",
    "                \n",
    "        # filter for online learning\n",
    "        self.RLSfilter = RLSFilterAnalyticIntercept(self.n_reservoir+self.n_inputs+self.input_bias, self.n_outputs, forgetting_factor=0.995)\n",
    "          \n",
    "        \n",
    "    def initialise_reservoir(self):\n",
    "        \n",
    "        # [nk] following Jaeger's paper:\n",
    "        W = np.random.uniform(low = -1, high = 1, size = (self.n_reservoir, self.n_reservoir))\n",
    "        # delete the fraction of connections given by (self.sparsity):\n",
    "        W[np.random.RandomState().rand(*W.shape) < self.sparsity] = 0\n",
    "        # compute the spectral radius of these weights:\n",
    "        radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        # rescale them to reach the requested spectral radius:\n",
    "        # if radius = 0, reinitialise weights again randomly\n",
    "        try:\n",
    "            W = W * (self.spectral_radius / radius)\n",
    "        except:\n",
    "            self.initialise_reservoir()\n",
    "              \n",
    "        return W\n",
    "    \n",
    "    def resetState(self):\n",
    "        self.laststate = np.zeros(self.n_reservoir)\n",
    "        self.lastextendedstate = np.zeros(self.n_reservoir+self.n_inputs)\n",
    "        self.lastinput = np.zeros(self.n_inputs)\n",
    "        self.lastoutput = np.zeros(self.n_inputs)\n",
    "\n",
    "    def _update(self, state, input_pattern, output_pattern=None):\n",
    "        \"\"\"performs one update step.\n",
    "        i.e., computes the next network state by applying the recurrent weights\n",
    "        to the last state & and feeding in the current input and output patterns\n",
    "        \"\"\"\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        if self.teacher_forcing:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern)\n",
    "                             + np.dot(self.W_feedb, output_pattern)\n",
    "                             )\n",
    "        else:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern)\n",
    "                             )\n",
    "            \n",
    "        # [nk] add noise - the original code added noise after applying non-linearity!\n",
    "        preactivation = preactivation + self.noise * (np.random.uniform(0,1,self.n_reservoir))\n",
    "        \n",
    "        # apply activation function to the reservoir with the necessary gain and the bias = -0.5*W\n",
    "        activation = sigmoid(preactivation,4) + self.reservoir_bias\n",
    "        \n",
    "        return activation\n",
    "\n",
    "    def _scale_inputs(self, inputs):\n",
    "        \"\"\"for each input dimension j: multiplies by the j'th entry in the\n",
    "        input_scaling argument, then adds the j'th entry of the input_shift\n",
    "        argument.\"\"\"\n",
    "        if self.input_scaling is not None:\n",
    "            inputs = np.dot(inputs, np.diag(self.input_scaling))\n",
    "        if self.input_shift is not None:\n",
    "            inputs = inputs + self.input_shift\n",
    "        return inputs\n",
    "\n",
    "    def _scale_teacher(self, teacher):\n",
    "        \"\"\"multiplies the teacher/target signal by the teacher_scaling argument,\n",
    "        then adds the teacher_shift argument to it.\"\"\"\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher = teacher * self.teacher_scaling\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher = teacher + self.teacher_shift\n",
    "        return teacher\n",
    "\n",
    "    def _unscale_teacher(self, teacher_scaled):\n",
    "        \"\"\"inverse operation of the _scale_teacher method.\"\"\"\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher_scaled = teacher_scaled - self.teacher_shift\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher_scaled = teacher_scaled / self.teacher_scaling\n",
    "        return teacher_scaled\n",
    "    \n",
    "    \n",
    "    def get_states(self, inputs, extended, continuation, outputs=None, inspect=False):\n",
    "        \"\"\"\n",
    "        [nk]\n",
    "        Collect the network's neuron activations.\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
    "            outputs: array of dimension (N_training_samples x n_outputs)\n",
    "            inspect: show a visualisation of the collected reservoir states\n",
    "        Returns:\n",
    "            the network's states for every input sample\n",
    "        \"\"\"\n",
    "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
    "        if inputs.ndim < 2:\n",
    "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
    "        \n",
    "        if outputs is not None and outputs.ndim < 2:\n",
    "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
    "\n",
    "        n_samples = inputs.shape[0]\n",
    "        \n",
    "        # add bias to inputs if there is such\n",
    "        if self.input_bias != 0:\n",
    "            inputs = np.hstack((inputs, np.ones((n_samples,1))))\n",
    "            \n",
    "        # use last state, input, output\n",
    "        if continuation:\n",
    "            laststate = self.laststate\n",
    "            lastinput = self.lastinput\n",
    "            lastoutput = self.lastoutput\n",
    "        else:\n",
    "            laststate = np.zeros(self.n_reservoir)\n",
    "            lastinput = np.zeros(self.n_inputs+self.input_bias)\n",
    "            lastoutput = np.zeros(self.n_outputs)\n",
    "            \n",
    "        if not self.silent:\n",
    "            print(\"harvesting states...\")    \n",
    "\n",
    "        # create scaled input and output vector\n",
    "        inputs_scaled = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
    "        if outputs is not None:\n",
    "            teachers_scaled = np.vstack([lastoutput, self._scale_teacher(outputs)])\n",
    "        # create states vector\n",
    "        states = np.vstack(\n",
    "            [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
    "        \n",
    "        \n",
    "        if self.augmented:\n",
    "            # create extended states vector\n",
    "            lastaugmentedstate = np.hstack((np.hstack((lastinput, laststate)),\n",
    "                                                        np.hstack((np.power(lastinput,2),np.power(laststate,2)))\n",
    "                                                        ))\n",
    "            augmented_states = np.vstack(\n",
    "                    [lastaugmentedstate,np.zeros((n_samples, self.n_reservoir*2+2))])\n",
    "            \n",
    "        \n",
    "        # activate the reservoir with the given input:\n",
    "        for n in range(1, n_samples+1):\n",
    "            if outputs is not None:\n",
    "                states[n, :] = self._update(states[n - 1], inputs_scaled[n, :],\n",
    "                                        teachers_scaled[n - 1, :])\n",
    "            else:\n",
    "                states[n, :] = self._update(states[n - 1], inputs_scaled[n, :])\n",
    "            \n",
    "            if self.augmented:\n",
    "                # x_squares(n) =  (u(n), x1(n), ... , xN(n), u^2(n), x1^2(n), ... , xN^2(n))\n",
    "                # ! teacher forcing version missing\n",
    "                augmented_states[n,:] = np.hstack((np.hstack((inputs_scaled[n,:],states[n,:])),\n",
    "                                                        np.hstack((np.power(inputs_scaled[n,:],2),np.power(states[n,:],2)))\n",
    "                                                        ))\n",
    "        # include the raw inputs for states\n",
    "        extended_states = np.hstack((inputs_scaled, states))\n",
    "        \n",
    "        # remember the last state, input, output for later:\n",
    "        self.laststate = states[-1, :]\n",
    "        self.lastextendedstate = extended_states[-1,:]\n",
    "        self.lastinput = inputs_scaled[-1, :]\n",
    "        if outputs is not None:\n",
    "            self.lastoutput = teachers_scaled[-1, :]\n",
    "        \n",
    "        # output states\n",
    "        if self.augmented:\n",
    "            out = augmented_states\n",
    "        elif extended:\n",
    "            out = extended_states\n",
    "        else:\n",
    "            out = states\n",
    "       \n",
    "        return out[1:]    #output without last state\n",
    "\n",
    "    def fit(self, outputs, inputs, continuation, inspect=False):\n",
    "        \"\"\"\n",
    "        [nk]\n",
    "        Collect the network's reaction to training data, train readout weights.\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
    "            outputs: array of dimension (N_training_samples x n_outputs)\n",
    "            inspect: show a visualisation of the collected reservoir states\n",
    "        Returns:\n",
    "            the network's output on the training data, using the trained weights\n",
    "        \"\"\"\n",
    "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
    "        if outputs.ndim < 2:\n",
    "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
    "        # transform teacher signal:\n",
    "        teachers_scaled = self._scale_teacher(outputs)\n",
    "        \n",
    "        # [nk] collect reservoir states\n",
    "        states = self.get_states(inputs, extended=True, continuation=continuation)\n",
    "\n",
    "        # learn the weights, i.e. find the linear combination of collected\n",
    "        # network states that is closest to the target output\n",
    "        if not self.silent:\n",
    "            print(\"fitting...\")\n",
    "        \n",
    "        # Solve for W_out:\n",
    "        if self.readout == 'pseudo-inverse':\n",
    "            self.W_out = np.dot(np.linalg.pinv(states[self.transient:, :]),\n",
    "                        self.inverse_out_activation(teachers_scaled[self.transient:, :])).T\n",
    "        elif self.readout == 'ridge':\n",
    "            self.readout = Ridge(alpha=self.ridge_reg)\n",
    "            self.readout.fit(states[self.transient:, :], teachers_scaled[self.transient:, :])\n",
    "        else:\n",
    "            raise ValueError('Invalid readout parameter: Must be either \"ridge\" or \"pseudo-inverse\".')\n",
    "\n",
    "        # optionally visualize the collected states\n",
    "        if inspect:\n",
    "            from matplotlib import pyplot as plt\n",
    "            # (^-- we depend on matplotlib only if this option is used)\n",
    "            plt.figure(\n",
    "                figsize=(states.shape[0] * 0.0025, states.shape[1] * 0.01))\n",
    "            plt.imshow(states.T, aspect='auto',\n",
    "                       interpolation='nearest')\n",
    "            plt.colorbar()\n",
    "\n",
    "        # apply learned weights to the collected states:\n",
    "        if not self.silent:\n",
    "            print(\"training (squared mean squared) error:\")\n",
    "        if self.readout == 'pseudo-inverse': \n",
    "            pred_train = self._unscale_teacher(self.out_activation(\n",
    "                    np.dot(states, self.W_out.T)))\n",
    "        else:   #ridge\n",
    "            pred_train = self._unscale_teacher(self.readout.predict(states))\n",
    "        if not self.silent:\n",
    "            print(np.sqrt(np.mean((pred_train - outputs)**2)))\n",
    "        return pred_train\n",
    "\n",
    "    \n",
    "    def predict(self, states):\n",
    "        \"\"\"\n",
    "        Apply the learned weights to the network's reactions to new input.\n",
    "        Args:\n",
    "            states: the reservoir of the network which has been activated by the input\n",
    "        Returns:\n",
    "            Array of output activations\n",
    "        \"\"\"\n",
    "        n_samples = states.shape[0]\n",
    "          \n",
    "        # output predictions for each input\n",
    "        outputs = np.zeros((n_samples, self.n_outputs))\n",
    "        for n in range(n_samples):\n",
    "            if self.readout == 'pseudo-inverse':\n",
    "                outputs[n, :] = self.out_activation(np.dot(self.W_out,states[n,:]))\n",
    "            else:   # ridge\n",
    "                outputs[n, :] = self.readout.predict(states[n,:].reshape(1,-1))\n",
    "        \n",
    "        unscaled_outputs = self._unscale_teacher(outputs)\n",
    "        \n",
    "        return unscaled_outputs \n",
    "    \n",
    "    # array to store all obs + rewards\n",
    "    def setHistory(self, episodes, steps):\n",
    "        num_elem_hist = self.n_inputs + self.n_outputs\n",
    "        self.history = np.repeat(-1, episodes*steps*num_elem_hist).reshape(episodes, steps, num_elem_hist)\n",
    "    \n",
    "    # internal reward: evaluate the current network on the history, fit it to the last teacher output from the history,\n",
    "    # evaluate the new network, return difference between the two\n",
    "    # Schmidhuber --> int_reward = C(p_old, hist) - C(p_new, hist)\n",
    "    def calculateInternalReward(self, allEpisodes=False):\n",
    "            \n",
    "        #------ calc C(p_old, hist)\n",
    "        num_elem_hist = self.n_inputs + self.n_outputs\n",
    "        hist = self.history[self.history != -1]  # take all time steps that happened\n",
    "        hist = hist.reshape(int(len(hist)/num_elem_hist),num_elem_hist) # reshape into (all time steps, hist elements)\n",
    "        \n",
    "        # take all history or last 10k times steps\n",
    "        if allEpisodes or hist.shape[0] <= self.defaultHistEval:\n",
    "            inputs = hist[:,:self.n_inputs]\n",
    "            teachers = hist[:,self.n_inputs:]\n",
    "        else:\n",
    "            inputs = hist[-self.defaultHistEval:,:self.n_inputs]\n",
    "            teachers = hist[-self.defaultHistEval:,self.n_inputs:]\n",
    "        \n",
    "        # apply inverse ou activation to the teacher signal\n",
    "        teachers = self.inverse_out_activation(teachers)\n",
    "        \n",
    "        # get reservoir activations for all history\n",
    "        res_states = self.get_states(inputs, extended=True, continuation=False)  #continuation is False because starts from first state\n",
    "        \n",
    "        # get predictions by applying the rls filter (without applying activation function)\n",
    "        preds1 = np.zeros((inputs.shape[0], self.n_outputs))\n",
    "        for i in range(inputs.shape[0]):\n",
    "            preds1[i,:] = self.out_activation(self.RLSfilter.predict(res_states[i,:].reshape(-1,1))).T       \n",
    "       \n",
    "        # calculate predictor quality\n",
    "        quality1 = np.sqrt(np.mean((preds1 - teachers)**2))\n",
    "        \n",
    "        #--------- update filter with last input-output\n",
    "        self.RLSfilter.process_datum(res_states[-1,:].reshape(-1,1), teachers[-1,:].reshape(-1,1))\n",
    "        \n",
    "        #------- calc C(p_new, hist)\n",
    "        preds2 = np.zeros((inputs.shape[0], self.n_outputs))\n",
    "        for i in range(inputs.shape[0]):\n",
    "            preds2[i,:] = self.RLSfilter.predict(res_states[i,:].reshape(-1,1)).T     \n",
    "       \n",
    "        # calculate predictor quality and save it\n",
    "        self.quality = np.sqrt(np.mean((preds2 - teachers)**2))\n",
    "\n",
    "        \n",
    "        return (quality1 - self.quality)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for ising taken from Aguilera's code for \"Adaptation to Criticality\":\n",
    "Changes:\n",
    "- Restricted - connections within hidden layer not being updated\n",
    "- Embodiment for environment Frozen Lake\n",
    "- RL Learning by a variant of SARSA - to Hinton's paper \"Using Q-energies\"\n",
    "     *using sensors and motors variables (not the state variable)\n",
    "     \n",
    "For Otsuka's model architecture:\n",
    "- Memory layer\n",
    "- Predictor ESN as parameter so it can have access to its reservoir and history\n",
    "- Sensors are 0/1 for SARSA learning and -1/1 for critical learning\n",
    "- SARSA learning includes updating the predictor's history and getting its internal reward\n",
    "\n",
    "The rule update is episodic!!\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class ising:\n",
    "    # Initialize the network\n",
    "    def __init__(self, netsize, Nmemory, Nsensors=1, Nmotors=1, predictor=None):  # Create ising model\n",
    "\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        self.size = netsize    #Network size\n",
    "        self.Ssize = Nsensors  # Number of sensors\n",
    "        self.Msize = Nmotors  # Number of motors\n",
    "        self.Memsize = Nmemory # Number of memory units\n",
    "        self.Inpsize = Nsensors + Nmemory  # Number of sensors+memory\n",
    "\n",
    "        self.h = np.zeros(netsize) # local biases\n",
    "        self.J = np.random.normal(0, 0.1, (self.size, self.size)) # symmetic weights between hidden variables\n",
    "        \n",
    "        if predictor is not None:\n",
    "            self.predictor = predictor\n",
    "\n",
    "        self.env = gym.make('FrozenLake8x8-v0')\n",
    "        self.observation = self.env.reset()\n",
    "        self.maxobs = 64   # !!!!!! For frozen Lake\n",
    "        self.numact = 4\n",
    "        \n",
    "        self.defaultGamma = 0.999  # as in Otsuka - Solving POMPDs\n",
    "        self.defaultLr = 0.01  # as in Otsuka - Solving POMPDs\n",
    "   \n",
    "\n",
    "    def get_state(self, mode='all'):\n",
    "        if mode == 'all':\n",
    "            return self.s\n",
    "        elif mode == 'motors':\n",
    "            return self.s[-self.Msize:]\n",
    "        elif mode == 'sensors': \n",
    "            return self.s[0:self.Inpsize]\n",
    "        elif mode == 'input':\n",
    "            return self.sensors\n",
    "        elif mode == 'non-sensors':\n",
    "            return self.s[self.Inpsize:]\n",
    "        elif mode == 'hidden':\n",
    "            return self.s[self.Inpsize:-self.Msize]\n",
    "\n",
    "    # gets the index of the configuration of the neurons - state as one number\n",
    "    def get_state_index(self, mode='all'):\n",
    "        return bool2int(0.5 * (self.get_state(mode) + 1))\n",
    "\n",
    "\n",
    "    # Set random bias to sets of units of the system\n",
    "    # bias to hidden and action\n",
    "    def random_fields(self, max_weights=None):\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        self.h[self.Inpsize:] = max_weights * \\\n",
    "            (np.random.rand(self.size - self.Inpsize) * 2 - 1)\n",
    "\n",
    "    # Set random connections to sets of units of the system\n",
    "    def random_wiring(self, max_weights=None):  # Set random values for J\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        for i in range(self.size):\n",
    "            for j in np.arange(i + 1, self.size):\n",
    "                if i < j and (i >= self.Inpsize or j >= self.Inpsize):  # don't add connections between the sensors\n",
    "                    # what about the motors? connection between the two will be added: doesn't matter because the correlations are not considered?\n",
    "                    self.J[i, j] = (np.random.rand(1) * 2 - 1) * self.max_weights\n",
    "        # get rid of connections between hidden units, and also motor (still dont know why this hasnt been done)\n",
    "        self.J[self.Inpsize:-self.Msize,self.Inpsize:-self.Msize] = 0  # between hidden\n",
    "        self.J[:self.Inpsize,-self.Msize:] = 0             # between sensor and motor\n",
    "        self.J[-self.Msize:,-self.Msize:] = 0            # between motor\n",
    "\n",
    "\n",
    "    # Transorm the sensor/motor input into integer index\n",
    "    def InputToIndex(self, x, xmax, bitsize):\n",
    "        return int(np.floor((x + xmax) / (2 * xmax + 10 * np.finfo(float).eps) * 2**bitsize))\n",
    "       \n",
    "    # Create state nodes for ESN to be observation from env + predictor's memory\n",
    "    # observation is binarised {0; 1}, linear memory nodes are used directly\n",
    "    def createJointInput(self, state, memory):\n",
    "        inp = np.zeros(self.Inpsize)\n",
    "        inp[:self.Ssize] = bitfield(self.InputToIndex(state, self.maxobs, self.Ssize), self.Ssize) # binarise observation\n",
    "        inp[self.Ssize:self.Inpsize] = memory  # use memory directly - better performance\n",
    "        return inp\n",
    "\n",
    "           \n",
    "    # Calculate hat_h_k for each k (expert)    \n",
    "    def ExpectedValueExperts(self, sensors, motors):\n",
    "        # self.s[Ssize:-Msize] - all hidden\n",
    "        # self.h[Ssize:-Msize] - biases of hidden\n",
    "        # self.J[:Ssize,Ssize:-Msize] - sensor-hidden connections\n",
    "        # self.J[Ssize:-Msize, -Msize:] - hidden-motor conn.\n",
    "        \n",
    "        num_e = self.size - (self.Inpsize+self.Msize)  # number of hidden units (experts)\n",
    "        ve = np.zeros(num_e)             # array of expected values for experts\n",
    "        for k in range(num_e):\n",
    "            ve[k] = sigmoid(np.dot(self.J[0:self.Inpsize,self.Inpsize:-self.Msize][:,k], sensors) + \n",
    "                            np.dot(self.J[self.Inpsize:-self.Msize, -self.Msize:][k,:], motors) +\n",
    "                            self.h[self.Inpsize:-self.Msize][k])\n",
    "        \n",
    "        return ve\n",
    "    \n",
    "    # takes in state and action and uses their binary equivalents to calculate the energy (not the actual network's)\n",
    "    # state_bit = binarised observation + memory\n",
    "    # action = the int action\n",
    "    def CalcFreeEnergy(self, state_bit, action):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "          \n",
    "        #binarise action\n",
    "        action_bit = action2bit(action, self.numact)\n",
    "    \n",
    "        ve = self.ExpectedValueExperts(state_bit, action_bit).reshape(-1,1)  #calculate expected hidden values\n",
    "        ss = state_bit.reshape(1,-1)\n",
    "        sm = action_bit.reshape(1,-1) \n",
    "        \n",
    "        # calculate the negative log-likelihood\n",
    "        a = -np.dot(np.dot(ss, self.J[:self.Inpsize,self.Inpsize:-self.Msize]), ve)    # 1\n",
    "        b = -np.dot(ss, self.h[:self.Inpsize].reshape(-1,1))                             # 2\n",
    "        c = -np.dot(np.dot(sm, np.transpose(self.J[self.Inpsize:-self.Msize, -self.Msize:])), ve)  # 3\n",
    "        d = -np.dot(sm, self.h[-self.Msize:].reshape(-1,1))                            # 4\n",
    "        e = -np.dot(self.h[self.Inpsize:-self.Msize], ve)                       # 5\n",
    "        \n",
    "        # calculate the negative entropy\n",
    "        f = 0                                                           # 6 + 7\n",
    "        for k in range(len(ve)):\n",
    "            f += ve[k]*np.log(ve[k] ) + (1 - ve[k])*np.log(1 - ve[k])\n",
    "        \n",
    "        return float(a+b+c+d+e+f)\n",
    "    \n",
    "    def SarsaLearning(self, total_episodes, max_steps, Beta, gamma=None, lr=None):\n",
    "        \n",
    "        pdb.set_trace()\n",
    "\n",
    "        self.predictor.setHistory(total_episodes, max_steps)  # create history array \n",
    "        self.log = np.tile(np.repeat(-1.0,7),(total_episodes, 1))  # track learning\n",
    "        \n",
    "        for episode in range(total_episodes):\n",
    "            \n",
    "            beta = Beta[episode]\n",
    "            \n",
    "            #memory is not reset per episode\n",
    "            if episode == 0:\n",
    "                memory = self.predictor.laststate\n",
    "            else:\n",
    "                memory = memory2\n",
    "            \n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # choose a (action) based on s = obs+m (obs+memory)\n",
    "            state_memory = self.createJointInput(state, memory)\n",
    "            action = self.ChooseAction(state_memory, beta)\n",
    "           \n",
    "            # calculate Q(s,a)\n",
    "            Q1 = -1*self.CalcFreeEnergy(state_memory, action)\n",
    "\n",
    "            self.initialiseDeltas()  # update of senhid weights, hidmot weights and the biases for the sen, mot and hid\n",
    "            Qs = np.repeat(-np.inf, max_steps)\n",
    "            PredQual = np.repeat(-np.inf, max_steps)\n",
    "            int_rew = np.repeat(-np.inf, max_steps)\n",
    "            \n",
    "            t = 0\n",
    "            while t < max_steps:             \n",
    "                \n",
    "                # calculate m'\n",
    "#                esn_input = np.hstack([state_bit, action_bit]).reshape(1,-1)\n",
    "                esn_input = np.array([state, action]).reshape(1,-1)\n",
    "                memory2 = self.predictor.get_states(esn_input, extended=False, continuation=True) # we only take state activations, not concatenate states with input which is done to train the weights and to also predict                                                                           \n",
    "                \n",
    "                # step with a and receive obs'\n",
    "                state2, ext_reward, done, info = self.env.step(action)\n",
    "                \n",
    "                #--\n",
    "                \n",
    "                # Predictor improvement and calculation of internal reward\n",
    "                self.predictor.history[episode,t,:] = np.array([state, action, state2, ext_reward])  # update predictor history\n",
    "                int_reward = np.tanh(self.predictor.calculateInternalReward()) # calculate internal reward\n",
    "            \n",
    "                if int_reward < 0:\n",
    "                    int_reward = 0\n",
    "                # Calculation of reward for SARSA update\n",
    "                reward = int_reward+ext_reward  # reward used for SARSA update\n",
    "#                reward = int_reward\n",
    "                \n",
    "                #--\n",
    "                \n",
    "                # choose a' based on s' = obs'+m'\n",
    "                state_memory2 = self.createJointInput(state2, memory2) # add the predictor's state to the obsrvations\n",
    "                action2 = self.ChooseAction(state_memory2, beta)\n",
    "                \n",
    "                # calculate Q(s',a')\n",
    "                Q2 = -1*self.CalcFreeEnergy(state_memory2, action2) # calculate Q2 = Q(s',a') = -F(s',a')\n",
    "                \n",
    "                # calculate update to the weights of the network currently having s and a\n",
    "                self.SarsaUpdateOnline(Q1, reward, Q2, state_memory, action, gamma, lr)\n",
    "                \n",
    "                Qs[t] = Q1\n",
    "                PredQual[t] = self.predictor.quality\n",
    "                int_rew[t] = int_reward\n",
    "\n",
    "                # updates for loop\n",
    "                state_memory = state_memory2\n",
    "                state = state2  # update obs = obs'   (needed to calculate m' = obs + a)\n",
    "                action = action2  # update a = a'  (needed to calculate m' = obs + a and to also get obs')\n",
    "                Q1 = Q2  # no need to update m (memory), because Q1 is directly updated\n",
    "\n",
    "                t += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "            # apply episodic update rule: t+1 is the number of time steps out of the max actually performed\n",
    "            self.SarsaUpdateEpisodic(t)\n",
    "            \n",
    "            # update log\n",
    "            vishidJ = np.vstack((self.J[:self.Inpsize,self.Inpsize:-self.Msize], np.transpose(self.J[self.Inpsize:-self.Msize, -self.Msize:])))\n",
    "            self.log[episode, :] = np.array([state, ext_reward, np.mean(PredQual[:t]), np.mean(Qs[:t]), np.mean(int_rew[:t]),\n",
    "                    np.mean(np.abs(vishidJ)), np.max(np.abs(vishidJ))])\n",
    "            \n",
    "\n",
    "    def SarsaUpdateOnline(self, Q1, reward, Q2, state_bit, action, gamma, lr):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        if gamma is None:\n",
    "            gamma = self.defaultGamma\n",
    "        if lr is None:\n",
    "            lr = self.defaultLr\n",
    "        \n",
    "        # TD error\n",
    "        rDiff = lr*(reward + gamma * Q2 - Q1)\n",
    "        \n",
    "        action_bit = action2bit(action, self.numact)\n",
    "        # calculate dQ/Wsensors\n",
    "        ve = self.ExpectedValueExperts(state_bit, action_bit).reshape(1,-1)\n",
    "        ve_new = np.repeat(ve, len(state_bit)).reshape(-1,1)\n",
    "        s_ne = np.repeat(state_bit.reshape(1,-1), ve.shape[1], axis=0)\n",
    "        s_new = s_ne.reshape(s_ne.shape[0]*s_ne.shape[1],1)\n",
    "        dQdWs = np.multiply(ve_new,s_new)\n",
    "        \n",
    "        # calculate dQ/Wmotors\n",
    "        ve_new = np.repeat(ve, len(action_bit)).reshape(-1,1)\n",
    "        m_ne = np.repeat(action_bit.reshape(1,-1), ve.shape[1], axis=0)\n",
    "        m_new = m_ne.reshape(m_ne.shape[0]*m_ne.shape[1],1)\n",
    "        dQdWm = np.multiply(ve_new,m_new)\n",
    "        \n",
    "        # updates weights of sensors and motors\n",
    "        num_h = self.size - (self.Inpsize+self.Msize)\n",
    "        self.dWs += (rDiff*dQdWs).reshape(num_h, self.Inpsize)\n",
    "        self.dWm += (rDiff*dQdWm).reshape(num_h, self.Msize)\n",
    "        \n",
    "        # updtes biases of sensors, motors, hidden\n",
    "        self.dhs += rDiff*state_bit\n",
    "        self.dhm += rDiff*action_bit\n",
    "        self.dhh += rDiff*(ve.reshape(-1))\n",
    "    \n",
    "    def SarsaUpdateEpisodic(self, T, lr=None):\n",
    "        \n",
    "        if lr is None:\n",
    "            lr = self.defaultLr\n",
    "        \n",
    "        # update weights\n",
    "        self.J[:self.Inpsize,self.Inpsize:-self.Msize] += lr*(self.dWs/T).T\n",
    "        self.J[self.Inpsize:-self.Msize, -self.Msize:] += lr*(self.dWm/T)\n",
    "        \n",
    "        # update biases\n",
    "        self.h[:self.Inpsize] += lr*(self.dhs/T)\n",
    "        self.h[-self.Msize:] += lr*(self.dhm/T)\n",
    "        self.h[self.Inpsize:-self.Msize] += lr*(self.dhh/T)\n",
    "        \n",
    "    \n",
    "    def initialiseDeltas(self):\n",
    "        num_h = self.size - (self.Inpsize+self.Msize)\n",
    "        \n",
    "        self.dWs = np.zeros((num_h, self.Inpsize))\n",
    "        self.dWm = np.zeros((num_h, self.Msize))\n",
    "        self.dhs = np.zeros(self.Inpsize)\n",
    "        self.dhm = np.zeros(self.Msize)\n",
    "        self.dhh = np.zeros(num_h)\n",
    "    \n",
    "    # state = observation + memory\n",
    "    def ChooseAction(self, state, beta):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        try:\n",
    "            # calculate probabilities of all actions - based on Otsuka's paper\n",
    "            p_a = np.zeros((self.env.nA,2))\n",
    "            for a in range(self.env.nA):\n",
    "                fa = np.exp(-beta*self.CalcFreeEnergy(state, a))\n",
    "                fa_ = 0\n",
    "                for a_ in range(self.env.nA):\n",
    "                    fa_ = fa_ + np.exp(-beta*self.CalcFreeEnergy(state, a_))\n",
    "                p_a[a,1] = fa/fa_\n",
    "                p_a[a,0] = a  #add index column\n",
    "\n",
    "            # sample an action from the distribution\n",
    "            ord_p_a = p_a[p_a[:,1].argsort()]\n",
    "            ord_p_a[:,1] = np.cumsum(ord_p_a[:,1])\n",
    "\n",
    "            b = np.array(ord_p_a[:,1] > np.random.rand())\n",
    "            act = int(ord_p_a[b.argmax(),0])  # take the index of the chosen action\n",
    "        \n",
    "        except RuntimeWarning:\n",
    "            pdb.set_trace()\n",
    " \n",
    "        return act\n",
    "    \n",
    "    def displayRunData(self, total_episodes, num_el):\n",
    "\n",
    "        res = self.log\n",
    "    \n",
    "        # pred errors\n",
    "    \n",
    "        x = range(total_episodes)\n",
    "        y = res[:,2]\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        line1, = ax.plot(x, y, label='mean predictor error')\n",
    "    \n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        # int reward\n",
    "    \n",
    "        y = res[:,4]\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        line1, = ax.plot(x, y, label='mean internal reward')\n",
    "    \n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        # actor -free energy\n",
    "    \n",
    "        y = res[:,3]\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        line1, = ax.plot(x, y, label='mean negative free energy')\n",
    "    \n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        # mean J\n",
    "    \n",
    "        y = res[:,5]\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        line1, = ax.plot(x, y, label='mean J')\n",
    "    \n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        # max J\n",
    "    \n",
    "        y = res[:,6]\n",
    "    \n",
    "        fig, ax = plt.subplots()\n",
    "        line1, = ax.plot(x, y, label='max J')\n",
    "    \n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "# Transform bool array into positive integer\n",
    "# [nk] encodes the state of the neurons as one number\n",
    "def bool2int(x):\n",
    "    y = 0\n",
    "    for i, j in enumerate(np.array(x)[::-1]):\n",
    "        y += j * 2**i\n",
    "    return int(y)\n",
    "\n",
    "# Transform positive integer into bit array\n",
    "def bitfield(n, size):\n",
    "    x = [int(x) for x in bin(int(n))[2:]]\n",
    "    x = [0] * (size - len(x)) + x\n",
    "    return np.array(x)\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# actions to be converted to a discrete multinomial variable\n",
    "def action2bit(action, max_act):\n",
    "    bit_action = np.zeros(max_act)\n",
    "    bit_action[action] = 1\n",
    "    \n",
    "    return bit_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-68-e2b230bf5260>(158)SarsaLearning()\n",
      "-> self.predictor.setHistory(total_episodes, max_steps)  # create history array\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(159)SarsaLearning()\n",
      "-> self.log = np.tile(np.repeat(-1.0,7),(total_episodes, 1))  # track learning\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(161)SarsaLearning()\n",
      "-> for episode in range(total_episodes):\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(163)SarsaLearning()\n",
      "-> beta = Beta[episode]\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(166)SarsaLearning()\n",
      "-> if episode == 0:\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(167)SarsaLearning()\n",
      "-> memory = self.predictor.laststate\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(172)SarsaLearning()\n",
      "-> state = self.env.reset()\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(175)SarsaLearning()\n",
      "-> state_memory = self.createJointInput(state, memory)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(176)SarsaLearning()\n",
      "-> action = self.ChooseAction(state_memory, beta)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(179)SarsaLearning()\n",
      "-> Q1 = -1*self.CalcFreeEnergy(state_memory, action)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(181)SarsaLearning()\n",
      "-> self.initialiseDeltas()  # update of senhid weights, hidmot weights and the biases for the sen, mot and hid\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(182)SarsaLearning()\n",
      "-> Qs = np.repeat(-np.inf, max_steps)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(183)SarsaLearning()\n",
      "-> PredQual = np.repeat(-np.inf, max_steps)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(184)SarsaLearning()\n",
      "-> int_rew = np.repeat(-np.inf, max_steps)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(186)SarsaLearning()\n",
      "-> t = 0\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(187)SarsaLearning()\n",
      "-> while t < max_steps:\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(191)SarsaLearning()\n",
      "-> esn_input = np.array([state, action]).reshape(1,-1)\n",
      "(Pdb) n\n",
      "> <ipython-input-68-e2b230bf5260>(192)SarsaLearning()\n",
      "-> memory2 = self.predictor.get_states(esn_input, extended=False, continuation=True) # we only take state activations, not concatenate states with input which is done to train the weights and to also predict\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> <ipython-input-67-80e77e9440f2>(254)get_states()\n",
      "-> def get_states(self, inputs, extended, continuation, outputs=None, inspect=False):\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(266)get_states()\n",
      "-> if inputs.ndim < 2:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(269)get_states()\n",
      "-> if outputs is not None and outputs.ndim < 2:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(272)get_states()\n",
      "-> n_samples = inputs.shape[0]\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(275)get_states()\n",
      "-> if self.input_bias != 0:\n",
      "(Pdb) inputs\n",
      "array([[0, 0]], dtype=int64)\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(276)get_states()\n",
      "-> inputs = np.hstack((inputs, np.ones((n_samples,1))))\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(279)get_states()\n",
      "-> if continuation:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(280)get_states()\n",
      "-> laststate = self.laststate\n",
      "(Pdb) inputs\n",
      "array([[0., 0., 1.]])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(281)get_states()\n",
      "-> lastinput = self.lastinput\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(282)get_states()\n",
      "-> lastoutput = self.lastoutput\n",
      "(Pdb) lastinput\n",
      "array([0., 0., 0.])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(288)get_states()\n",
      "-> if not self.silent:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(292)get_states()\n",
      "-> inputs_scaled = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(293)get_states()\n",
      "-> if outputs is not None:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(296)get_states()\n",
      "-> states = np.vstack(\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(297)get_states()\n",
      "-> [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(300)get_states()\n",
      "-> if self.augmented:\n",
      "(Pdb) states\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(310)get_states()\n",
      "-> for n in range(1, n_samples+1):\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(311)get_states()\n",
      "-> if outputs is not None:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(315)get_states()\n",
      "-> states[n, :] = self._update(states[n - 1], inputs_scaled[n, :])\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> <ipython-input-67-80e77e9440f2>(200)_update()\n",
      "-> def _update(self, state, input_pattern, output_pattern=None):\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(208)_update()\n",
      "-> if self.teacher_forcing:\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(214)_update()\n",
      "-> preactivation = (np.dot(self.W, state)\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(215)_update()\n",
      "-> + np.dot(self.W_in, input_pattern)\n",
      "(Pdb) np.dot(self.W, state)\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "(Pdb) np.dot(self.W_in, input_pattern)\n",
      "array([-0.0899722 , -0.0708052 ,  0.07083832,  0.0611418 , -0.03735992,\n",
      "        0.02919159, -0.06495181,  0.01682972,  0.04280267,  0.02253906,\n",
      "       -0.02358478, -0.07088169])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(219)_update()\n",
      "-> preactivation = preactivation + self.noise * (np.random.uniform(0,1,self.n_reservoir))\n",
      "(Pdb) preactivation\n",
      "array([-0.0899722 , -0.0708052 ,  0.07083832,  0.0611418 , -0.03735992,\n",
      "        0.02919159, -0.06495181,  0.01682972,  0.04280267,  0.02253906,\n",
      "       -0.02358478, -0.07088169])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(222)_update()\n",
      "-> activation = sigmoid(preactivation,4) + self.reservoir_bias\n",
      "(Pdb) preactivation\n",
      "array([-0.08987607, -0.07034065,  0.07149483,  0.06148854, -0.03673622,\n",
      "        0.02923524, -0.06488721,  0.01749907,  0.04328311,  0.02290637,\n",
      "       -0.02313007, -0.07033869])\n",
      "(Pdb) sigmoid(preactivation,4)\n",
      "array([0.41107957, 0.43011975, 0.57101152, 0.56118043, 0.46332974,\n",
      "       0.52920197, 0.43547462, 0.51749193, 0.54317532, 0.52289036,\n",
      "       0.47688641, 0.43012167])\n",
      "(Pdb) self.reservoir_bias\n",
      "array([ 0.72945285,  0.        ,  0.1430407 ,  0.        ,  0.06361378,\n",
      "        0.0525488 , -0.60140485, -0.44714229,  0.        ,  0.        ,\n",
      "       -0.68444341, -0.00135588])\n",
      "(Pdb) n\n",
      "> <ipython-input-67-80e77e9440f2>(224)_update()\n",
      "-> return activation\n",
      "(Pdb) activation\n",
      "array([ 1.14053242,  0.43011975,  0.71405222,  0.56118043,  0.52694352,\n",
      "        0.58175077, -0.16593023,  0.07034964,  0.54317532,  0.52289036,\n",
      "       -0.20755699,  0.42876579])\n",
      "(Pdb) self.reservoir_bias\n",
      "array([ 0.72945285,  0.        ,  0.1430407 ,  0.        ,  0.06361378,\n",
      "        0.0525488 , -0.60140485, -0.44714229,  0.        ,  0.        ,\n",
      "       -0.68444341, -0.00135588])\n",
      "(Pdb) preactivation = np.repeat(0.5, 12)\n",
      "(Pdb) preactivation\n",
      "array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
      "(Pdb) preactivation2 = np.dot(self.W,preactivation)\n",
      "(Pdb) preactivation2\n",
      "array([-0.436299  ,  0.        ,  0.60144841,  0.        , -0.24692307,\n",
      "        0.44714229,  0.        , -0.36454215,  0.3837502 ,  0.05003479,\n",
      "        0.        ,  0.31107883])\n",
      "(Pdb) sigmoid(preactivation,4)\n",
      "array([0.88079708, 0.88079708, 0.88079708, 0.88079708, 0.88079708,\n",
      "       0.88079708, 0.88079708, 0.88079708, 0.88079708, 0.88079708,\n",
      "       0.88079708, 0.88079708])\n",
      "(Pdb) sigmoid(preactivation,4) + self.reservoir_bias\n",
      "array([1.61024993, 0.88079708, 1.02383778, 0.88079708, 0.94441086,\n",
      "       0.93334588, 0.27939223, 0.43365479, 0.88079708, 0.88079708,\n",
      "       0.19635367, 0.8794412 ])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),4) + repeat(0.5,3)\n",
      "*** NameError: name 'repeat' is not defined\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),4) + np.repeat(0.5,3)\n",
      "array([1.38079708, 1.38079708, 1.38079708])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),4) - np.repeat(0.5,3)\n",
      "array([0.38079708, 0.38079708, 0.38079708])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),1) + repeat(0.5,3)\n",
      "*** NameError: name 'repeat' is not defined\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),1) - np.repeat(0.5,3)\n",
      "array([0.12245933, 0.12245933, 0.12245933])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),0.5) - np.repeat(0.5,3)\n",
      "array([0.0621765, 0.0621765, 0.0621765])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),1/4) - np.repeat(0.5,3)\n",
      "array([0.03120937, 0.03120937, 0.03120937])\n",
      "(Pdb) sigmoid(np.repeat(0.5,3),4)\n",
      "array([0.88079708, 0.88079708, 0.88079708])\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-66902d81dde8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Beta = np.linspace(0,15,1000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mI\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSarsaLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-e2b230bf5260>\u001b[0m in \u001b[0;36mSarsaLearning\u001b[1;34m(self, total_episodes, max_steps, Beta, gamma, lr)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;31m#                esn_input = np.hstack([state_bit, action_bit]).reshape(1,-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[0mesn_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 \u001b[0mmemory2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextended\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we only take state activations, not concatenate states with input which is done to train the weights and to also predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# step with a and receive obs'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-80e77e9440f2>\u001b[0m in \u001b[0;36mget_states\u001b[1;34m(self, inputs, extended, continuation, outputs, inspect)\u001b[0m\n\u001b[0;32m    313\u001b[0m                                         teachers_scaled[n - 1, :])\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                 \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugmented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-80e77e9440f2>\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, state, input_pattern, output_pattern)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreservoir_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_scale_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-80e77e9440f2>\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, state, input_pattern, output_pattern)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreactivation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreservoir_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_scale_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gym\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pdb\n",
    "\n",
    "# esn params\n",
    "predictor_inputs = 2 # linear state,action\n",
    "predictor_outputs = 2 # linear state2, reward\n",
    "predictor_reservoir = 12\n",
    "predictor_radius = 0.9\n",
    "predictor_sparsity = 0.9\n",
    "out_act = identity\n",
    "inv_out_act = identity\n",
    "\n",
    "#predictor\n",
    "predictor = ESN(n_inputs = predictor_inputs, n_outputs = predictor_outputs, n_reservoir=predictor_reservoir,\n",
    "                 spectral_radius=predictor_radius, sparsity=predictor_sparsity,\n",
    "                 out_activation=out_act, inverse_out_activation=inv_out_act,\n",
    "                input_bias = 1\n",
    "                 )\n",
    "\n",
    "# rbm params\n",
    "Ssize=7\n",
    "Msize=4\n",
    "Nhidden = 20\n",
    "Nmemory = predictor_reservoir\n",
    "size=Ssize+Nmemory+Nhidden+Msize\n",
    "\n",
    "#actor\n",
    "I = ising(size,Nmemory,Ssize,Msize,predictor)\n",
    "\n",
    "\n",
    "# SARSA RL\n",
    "total_episodes = 10000\n",
    "max_steps = 100         # in Otsuka they are 250 but it's a different task\n",
    "Beta = np.linspace(0,15,total_episodes)\n",
    "# Beta = np.repeat(1,30)\n",
    "# Beta = np.linspace(0,15,1000)\n",
    "\n",
    "I.SarsaLearning(total_episodes, max_steps, Beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
