{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "ESN Implementation imported from: https://github.com/cknd/pyESN/blob/master/pyESN.py\n",
    "\n",
    "Initialisation of reservoir weights and input weights done according to Jaeger's paper,\n",
    "and implementation of augmented training algorithm following the paper:\n",
    "    https://papers.nips.cc/paper/2318-adaptive-nonlinear-system-identification-with-echo-state-networks.pdf  \n",
    "\n",
    "Readout training with Moore-Penrose Matrix Inverse or Ridge Regression (added)\n",
    "\n",
    "Changes for Otsuka's model architecture:\n",
    "    - sigmoid function for update of memory layer (reservoir units): \n",
    "        gain=4, added bias for the reservoir activations\n",
    "    - default out activation: identity\n",
    "    - get_states method to work with continuation\n",
    "    - fit and predict methods take reservoir units (output from get_states) directly\n",
    "    - RLS regression for online learning\n",
    "    \n",
    "Intrinsic Motivation Schmidhuber\n",
    "    - history object: for each n in N_T (episodes*num_steps): [state, action, state2, reward]\n",
    "    - history is evaluated on the last <= 10000 time-steps\n",
    "    - calculateReward methods\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from cannon_rlsfilter import RLSFilterAnalyticIntercept\n",
    "import math\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"error\")\n",
    "\n",
    "\n",
    "\n",
    "def correct_dimensions(s, targetlength):\n",
    "    \"\"\"checks the dimensionality of some numeric argument s, broadcasts it\n",
    "       to the specified length if possible.\n",
    "    Args:\n",
    "        s: None, scalar or 1D array\n",
    "        targetlength: expected length of s\n",
    "    Returns:\n",
    "        None if s is None, else numpy vector of length targetlength\n",
    "    \"\"\"\n",
    "    if s is not None:\n",
    "        s = np.array(s)\n",
    "        if s.ndim == 0:\n",
    "            s = np.array([s] * targetlength)\n",
    "        elif s.ndim == 1:\n",
    "            if not len(s) == targetlength:\n",
    "                raise ValueError(\"arg must have length \" + str(targetlength))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x, gain=1):\n",
    "    return 1 / (1 + np.exp(-gain*x))\n",
    "\n",
    "# !!! do I apply the gain to the predictions as well?\n",
    "def inv_sigmoid(x, gain=1):\n",
    "    return np.log( (x*gain) / (1 - (x*gain) ) )\n",
    "\n",
    "def atanh(x):\n",
    "    #x is of shape (1, teachers)\n",
    "    \n",
    "    atanhx = np.zeros(x.shape[1])\n",
    "    for i,v in enumerate(x[0]):\n",
    "        atanhx[i] = math.atanh(v)\n",
    "        \n",
    "    return atanhx\n",
    "        \n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class ESN():\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs, n_reservoir=200,\n",
    "                 spectral_radius=0.95, sparsity=0,\n",
    "                 noise=0.001,\n",
    "                 readout='pseudo-inverse',\n",
    "                 ridge_reg=None,\n",
    "                 input_weights_scaling = 1,\n",
    "                 input_scaling=None,input_shift=None,teacher_forcing=None, feedback_scaling=None,\n",
    "                 teacher_scaling=None, teacher_shift=None,\n",
    "                 out_activation=np.tanh, inverse_out_activation=atanh,\n",
    "                 silent=True, \n",
    "                 augmented=False,\n",
    "                 transient=200,\n",
    "                 input_bias=0\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_inputs: nr of input dimensions\n",
    "            n_outputs: nr of output dimensions\n",
    "            n_reservoir: nr of reservoir neurons\n",
    "            spectral_radius: spectral radius of the recurrent weight matrix\n",
    "            sparsity: proportion of recurrent weights set to zero\n",
    "            noise: noise added to each neuron (regularization)\n",
    "            readout: type of readout 0 can be moonrose pseudo-inverse or ridge regression\n",
    "            ridge_reg: regularisation value alpha if readout is Ridge\n",
    "            \n",
    "            input_weights_scaling: scaling of the input connection weights\n",
    "            input_shift: scalar or vector of length n_inputs to add to each\n",
    "                        input dimension before feeding it to the network.                       \n",
    "            input_scaling: scalar or vector of length n_inputs to multiply\n",
    "                        with each input dimension before feeding it to the netw.\n",
    "                        \n",
    "            teacher_shift: additive term applied to the target signal\n",
    "            teacher_scaling: factor applied to the target signal\n",
    "            teacher_forcing: if True, feed the target back into output units\n",
    "\n",
    "            out_activation: output activation function (applied to the readout)\n",
    "            inverse_out_activation: inverse of the output activation function\n",
    "    \n",
    "            silent: supress messages\n",
    "            augmented: if True, use augmented training algorithm\n",
    "            transient: how many initial states to discard\n",
    "            \n",
    "        \"\"\"\n",
    "        # check for proper dimensionality of all arguments and write them down.\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_reservoir = n_reservoir\n",
    "        self.n_outputs = n_outputs   # part will be obs, part will be reward\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.sparsity = sparsity\n",
    "        self.noise = noise\n",
    "        self.readout = readout\n",
    "        self.ridge_reg = ridge_reg\n",
    "        \n",
    "        self.input_weights_scaling = input_weights_scaling\n",
    "        self.input_shift = correct_dimensions(input_shift, n_inputs)\n",
    "        self.input_scaling = correct_dimensions(input_scaling, n_inputs)\n",
    "\n",
    "        self.teacher_shift = teacher_shift\n",
    "        self.teacher_scaling = teacher_scaling\n",
    "        self.teacher_forcing = teacher_forcing\n",
    "\n",
    "        self.out_activation = out_activation\n",
    "        self.inverse_out_activation = inverse_out_activation\n",
    "\n",
    "        self.silent = silent\n",
    "        self.augmented = augmented\n",
    "        self.transient = transient\n",
    "        \n",
    "        self.input_bias = input_bias\n",
    "        \n",
    "        self.laststate = np.zeros(self.n_reservoir)\n",
    "        self.lastextendedstate = np.zeros(self.n_reservoir+self.n_inputs)\n",
    "        self.lastinput = np.zeros(self.n_inputs+self.input_bias)\n",
    "        self.lastoutput = np.zeros(self.n_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.defaultHistEval = 10000    # look at last n time steps when evaluating history\n",
    "        \n",
    "        self.initweights()\n",
    "        \n",
    "    def initweights(self):\n",
    "        \n",
    "        # initialize recurrent weights:\n",
    "        self.W = self.initialise_reservoir()\n",
    "        \n",
    "        # bias for the update of the states of the reservoir\n",
    "        self.reservoir_bias = np.dot(np.repeat(-0.5,self.n_reservoir),self.W)\n",
    "            \n",
    "        # [nk] following Jaeger's paper:\n",
    "        # added scaling\n",
    "        self.W_in = np.random.uniform(low = -0.1, high = 0.1, size = (self.n_reservoir, self.n_inputs+self.input_bias))*self.input_weights_scaling\n",
    "             \n",
    "        # random feedback (teacher forcing) weights:\n",
    "        self.W_feedb = np.random.RandomState().rand(\n",
    "            self.n_reservoir, self.n_outputs) * 2 - 1\n",
    "                \n",
    "        # filter for online learning\n",
    "        self.RLSfilter = RLSFilterAnalyticIntercept(self.n_reservoir+self.n_inputs+self.input_bias, self.n_outputs, forgetting_factor=0.995)\n",
    "          \n",
    "        \n",
    "    def initialise_reservoir(self):\n",
    "        \n",
    "        # [nk] following Jaeger's paper:\n",
    "        W = np.random.uniform(low = -1, high = 1, size = (self.n_reservoir, self.n_reservoir))\n",
    "        # delete the fraction of connections given by (self.sparsity):\n",
    "        W[np.random.RandomState().rand(*W.shape) < self.sparsity] = 0\n",
    "        # compute the spectral radius of these weights:\n",
    "        radius = np.max(np.abs(np.linalg.eigvals(W)))\n",
    "        # rescale them to reach the requested spectral radius:\n",
    "        # if radius = 0, reinitialise weights again randomly\n",
    "        try:\n",
    "            W = W * (self.spectral_radius / radius)\n",
    "        except:\n",
    "            self.initialise_reservoir()\n",
    "              \n",
    "        return W\n",
    "    \n",
    "    def resetState(self):\n",
    "        self.laststate = np.zeros(self.n_reservoir)\n",
    "        self.lastextendedstate = np.zeros(self.n_reservoir+self.n_inputs)\n",
    "        self.lastinput = np.zeros(self.n_inputs)\n",
    "        self.lastoutput = np.zeros(self.n_inputs)\n",
    "\n",
    "    def _update(self, state, input_pattern, output_pattern=None):\n",
    "        \"\"\"performs one update step.\n",
    "        i.e., computes the next network state by applying the recurrent weights\n",
    "        to the last state & and feeding in the current input and output patterns\n",
    "        \"\"\"\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        if self.teacher_forcing:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern)\n",
    "                             + np.dot(self.W_feedb, output_pattern)\n",
    "                             )\n",
    "        else:\n",
    "            preactivation = (np.dot(self.W, state)\n",
    "                             + np.dot(self.W_in, input_pattern)\n",
    "                             )\n",
    "            \n",
    "        # [nk] add noise - the original code added noise after applying non-linearity!\n",
    "        preactivation = preactivation + self.noise * (np.random.uniform(0,1,self.n_reservoir))\n",
    "        \n",
    "        # apply activation function to the reservoir with the necessary gain and the bias = -0.5*W\n",
    "        activation = sigmoid(preactivation,4) + self.reservoir_bias\n",
    "        \n",
    "        return activation\n",
    "\n",
    "    def _scale_inputs(self, inputs):\n",
    "        \"\"\"for each input dimension j: multiplies by the j'th entry in the\n",
    "        input_scaling argument, then adds the j'th entry of the input_shift\n",
    "        argument.\"\"\"\n",
    "        if self.input_scaling is not None:\n",
    "            inputs = np.dot(inputs, np.diag(self.input_scaling))\n",
    "        if self.input_shift is not None:\n",
    "            inputs = inputs + self.input_shift\n",
    "        return inputs\n",
    "\n",
    "    def _scale_teacher(self, teacher):\n",
    "        \"\"\"multiplies the teacher/target signal by the teacher_scaling argument,\n",
    "        then adds the teacher_shift argument to it.\"\"\"\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher = teacher * self.teacher_scaling\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher = teacher + self.teacher_shift\n",
    "        return teacher\n",
    "\n",
    "    def _unscale_teacher(self, teacher_scaled):\n",
    "        \"\"\"inverse operation of the _scale_teacher method.\"\"\"\n",
    "        if self.teacher_shift is not None:\n",
    "            teacher_scaled = teacher_scaled - self.teacher_shift\n",
    "        if self.teacher_scaling is not None:\n",
    "            teacher_scaled = teacher_scaled / self.teacher_scaling\n",
    "        return teacher_scaled\n",
    "    \n",
    "    \n",
    "    def get_states(self, inputs, extended, continuation, outputs=None, inspect=False):\n",
    "        \"\"\"\n",
    "        [nk]\n",
    "        Collect the network's neuron activations.\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
    "            outputs: array of dimension (N_training_samples x n_outputs)\n",
    "            inspect: show a visualisation of the collected reservoir states\n",
    "        Returns:\n",
    "            the network's states for every input sample\n",
    "        \"\"\"\n",
    "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
    "        if inputs.ndim < 2:\n",
    "            inputs = np.reshape(inputs, (len(inputs), -1))\n",
    "        \n",
    "        if outputs is not None and outputs.ndim < 2:\n",
    "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
    "\n",
    "        n_samples = inputs.shape[0]\n",
    "        \n",
    "        # add bias to inputs if there is such\n",
    "        if self.input_bias != 0:\n",
    "            inputs = np.hstack((inputs, np.ones((n_samples,1))))\n",
    "            \n",
    "        # use last state, input, output\n",
    "        if continuation:\n",
    "            laststate = self.laststate\n",
    "            lastinput = self.lastinput\n",
    "            lastoutput = self.lastoutput\n",
    "        else:\n",
    "            laststate = np.zeros(self.n_reservoir)\n",
    "            lastinput = np.zeros(self.n_inputs+self.input_bias)\n",
    "            lastoutput = np.zeros(self.n_outputs)\n",
    "            \n",
    "        if not self.silent:\n",
    "            print(\"harvesting states...\")    \n",
    "\n",
    "        # create scaled input and output vector\n",
    "        inputs_scaled = np.vstack([lastinput, self._scale_inputs(inputs)])\n",
    "        if outputs is not None:\n",
    "            teachers_scaled = np.vstack([lastoutput, self._scale_teacher(outputs)])\n",
    "        # create states vector\n",
    "        states = np.vstack(\n",
    "            [laststate, np.zeros((n_samples, self.n_reservoir))])\n",
    "        \n",
    "        \n",
    "        if self.augmented:\n",
    "            # create extended states vector\n",
    "            lastaugmentedstate = np.hstack((np.hstack((lastinput, laststate)),\n",
    "                                                        np.hstack((np.power(lastinput,2),np.power(laststate,2)))\n",
    "                                                        ))\n",
    "            augmented_states = np.vstack(\n",
    "                    [lastaugmentedstate,np.zeros((n_samples, self.n_reservoir*2+2))])\n",
    "            \n",
    "        \n",
    "        # activate the reservoir with the given input:\n",
    "        for n in range(1, n_samples+1):\n",
    "            if outputs is not None:\n",
    "                states[n, :] = self._update(states[n - 1], inputs_scaled[n, :],\n",
    "                                        teachers_scaled[n - 1, :])\n",
    "            else:\n",
    "                states[n, :] = self._update(states[n - 1], inputs_scaled[n, :])\n",
    "            \n",
    "            if self.augmented:\n",
    "                # x_squares(n) =  (u(n), x1(n), ... , xN(n), u^2(n), x1^2(n), ... , xN^2(n))\n",
    "                # ! teacher forcing version missing\n",
    "                augmented_states[n,:] = np.hstack((np.hstack((inputs_scaled[n,:],states[n,:])),\n",
    "                                                        np.hstack((np.power(inputs_scaled[n,:],2),np.power(states[n,:],2)))\n",
    "                                                        ))\n",
    "        # include the raw inputs for states\n",
    "        extended_states = np.hstack((inputs_scaled, states))\n",
    "        \n",
    "        # remember the last state, input, output for later:\n",
    "        self.laststate = states[-1, :]\n",
    "        self.lastextendedstate = extended_states[-1,:]\n",
    "        self.lastinput = inputs_scaled[-1, :]\n",
    "        if outputs is not None:\n",
    "            self.lastoutput = teachers_scaled[-1, :]\n",
    "        \n",
    "        # output states\n",
    "        if self.augmented:\n",
    "            out = augmented_states\n",
    "        elif extended:\n",
    "            out = extended_states\n",
    "        else:\n",
    "            out = states\n",
    "       \n",
    "        return out[1:]    #output without last state\n",
    "\n",
    "    def fit(self, outputs, inputs, continuation, inspect=False):\n",
    "        \"\"\"\n",
    "        [nk]\n",
    "        Collect the network's reaction to training data, train readout weights.\n",
    "        Args:\n",
    "            inputs: array of dimensions (N_training_samples x n_inputs)\n",
    "            outputs: array of dimension (N_training_samples x n_outputs)\n",
    "            inspect: show a visualisation of the collected reservoir states\n",
    "        Returns:\n",
    "            the network's output on the training data, using the trained weights\n",
    "        \"\"\"\n",
    "        # transform any vectors of shape (x,) into vectors of shape (x,1):\n",
    "        if outputs.ndim < 2:\n",
    "            outputs = np.reshape(outputs, (len(outputs), -1))\n",
    "        # transform teacher signal:\n",
    "        teachers_scaled = self._scale_teacher(outputs)\n",
    "        \n",
    "        # [nk] collect reservoir states\n",
    "        states = self.get_states(inputs, extended=True, continuation=continuation)\n",
    "\n",
    "        # learn the weights, i.e. find the linear combination of collected\n",
    "        # network states that is closest to the target output\n",
    "        if not self.silent:\n",
    "            print(\"fitting...\")\n",
    "        \n",
    "        # Solve for W_out:\n",
    "        if self.readout == 'pseudo-inverse':\n",
    "            self.W_out = np.dot(np.linalg.pinv(states[self.transient:, :]),\n",
    "                        self.inverse_out_activation(teachers_scaled[self.transient:, :])).T\n",
    "        elif self.readout == 'ridge':\n",
    "            self.readout = Ridge(alpha=self.ridge_reg)\n",
    "            self.readout.fit(states[self.transient:, :], teachers_scaled[self.transient:, :])\n",
    "        else:\n",
    "            raise ValueError('Invalid readout parameter: Must be either \"ridge\" or \"pseudo-inverse\".')\n",
    "\n",
    "        # optionally visualize the collected states\n",
    "        if inspect:\n",
    "            from matplotlib import pyplot as plt\n",
    "            # (^-- we depend on matplotlib only if this option is used)\n",
    "            plt.figure(\n",
    "                figsize=(states.shape[0] * 0.0025, states.shape[1] * 0.01))\n",
    "            plt.imshow(states.T, aspect='auto',\n",
    "                       interpolation='nearest')\n",
    "            plt.colorbar()\n",
    "\n",
    "        # apply learned weights to the collected states:\n",
    "        if not self.silent:\n",
    "            print(\"training (squared mean squared) error:\")\n",
    "        if self.readout == 'pseudo-inverse': \n",
    "            pred_train = self._unscale_teacher(self.out_activation(\n",
    "                    np.dot(states, self.W_out.T)))\n",
    "        else:   #ridge\n",
    "            pred_train = self._unscale_teacher(self.readout.predict(states))\n",
    "        if not self.silent:\n",
    "            print(np.sqrt(np.mean((pred_train - outputs)**2)))\n",
    "        return pred_train\n",
    "\n",
    "    \n",
    "    def predict(self, states):\n",
    "        \"\"\"\n",
    "        Apply the learned weights to the network's reactions to new input.\n",
    "        Args:\n",
    "            states: the reservoir of the network which has been activated by the input\n",
    "        Returns:\n",
    "            Array of output activations\n",
    "        \"\"\"\n",
    "        n_samples = states.shape[0]\n",
    "          \n",
    "        # output predictions for each input\n",
    "        outputs = np.zeros((n_samples, self.n_outputs))\n",
    "        for n in range(n_samples):\n",
    "            if self.readout == 'pseudo-inverse':\n",
    "                outputs[n, :] = self.out_activation(np.dot(self.W_out,states[n,:]))\n",
    "            else:   # ridge\n",
    "                outputs[n, :] = self.readout.predict(states[n,:].reshape(1,-1))\n",
    "        \n",
    "        unscaled_outputs = self._unscale_teacher(outputs)\n",
    "        \n",
    "        return unscaled_outputs \n",
    "    \n",
    "    # array to store all obs + rewards\n",
    "    def setHistory(self, episodes, steps):\n",
    "        num_elem_hist = self.n_inputs + self.n_outputs\n",
    "        self.history = np.repeat(-1, episodes*steps*num_elem_hist).reshape(episodes, steps, num_elem_hist)\n",
    "    \n",
    "    # internal reward: evaluate the current network on the history, fit it to the last teacher output from the history,\n",
    "    # evaluate the new network, return difference between the two\n",
    "    # Schmidhuber --> int_reward = C(p_old, hist) - C(p_new, hist)\n",
    "    def calculateInternalReward(self, allEpisodes=False):\n",
    "            \n",
    "        #------ calc C(p_old, hist)\n",
    "        num_elem_hist = self.n_inputs + self.n_outputs\n",
    "        hist = self.history[self.history != -1]  # take all time steps that happened\n",
    "        hist = hist.reshape(int(len(hist)/num_elem_hist),num_elem_hist) # reshape into (all time steps, hist elements)\n",
    "        \n",
    "        # take all history or last 10k times steps\n",
    "        if allEpisodes or hist.shape[0] <= self.defaultHistEval:\n",
    "            inputs = hist[:,:self.n_inputs]\n",
    "            teachers = hist[:,self.n_inputs:]\n",
    "        else:\n",
    "            inputs = hist[-self.defaultHistEval:,:self.n_inputs]\n",
    "            teachers = hist[-self.defaultHistEval:,self.n_inputs:]\n",
    "        \n",
    "        # apply inverse ou activation to the teacher signal\n",
    "        teachers = self.inverse_out_activation(teachers)\n",
    "        \n",
    "        # get reservoir activations for all history\n",
    "        res_states = self.get_states(inputs, extended=True, continuation=False)  #continuation is False because starts from first state\n",
    "        \n",
    "        # get predictions by applying the rls filter (without applying activation function)\n",
    "        preds1 = np.zeros((inputs.shape[0], self.n_outputs))\n",
    "        for i in range(inputs.shape[0]):\n",
    "            preds1[i,:] = self.out_activation(self.RLSfilter.predict(res_states[i,:].reshape(-1,1))).T       \n",
    "       \n",
    "        # calculate predictor quality\n",
    "        quality1 = np.sqrt(np.mean((preds1 - teachers)**2))\n",
    "        \n",
    "        #--------- update filter with last input-output\n",
    "        self.RLSfilter.process_datum(res_states[-1,:].reshape(-1,1), teachers[-1,:].reshape(-1,1))\n",
    "        \n",
    "        #------- calc C(p_new, hist)\n",
    "        preds2 = np.zeros((inputs.shape[0], self.n_outputs))\n",
    "        for i in range(inputs.shape[0]):\n",
    "            preds2[i,:] = self.RLSfilter.predict(res_states[i,:].reshape(-1,1)).T     \n",
    "       \n",
    "        # calculate predictor quality and save it\n",
    "        self.quality = np.sqrt(np.mean((preds2 - teachers)**2))\n",
    "\n",
    "        \n",
    "        return (quality1 - self.quality)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for ising taken from Aguilera's code for \"Adaptation to Criticality\":\n",
    "Changes:\n",
    "- Restricted - connections within hidden layer not being updated\n",
    "- Embodiment for environment Frozen Lake\n",
    "- RL Learning by a variant of SARSA - to Hinton's paper \"Using Q-energies\"\n",
    "     *using sensors and motors variables (not the state variable)\n",
    "     \n",
    "For Otsuka's model architecture:\n",
    "- Predictor ESN as parameter so it can have access to its reservoir and history\n",
    "- Memory layer: doesn't get reset throughout training\n",
    "- Inputs are binarised observation, memory, reward\n",
    "- An episode is T*10 time steps\n",
    "\n",
    "Intrinsic Motivation:\n",
    "- internal reward is 0 to 1\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "\n",
    "import pdb\n",
    "\n",
    "\n",
    "class ising:\n",
    "    # Initialize the network\n",
    "    def __init__(self, netsize, Nmemory, Nreward, Nsensors=1, Nmotors=1, predictor=None):  # Create ising model\n",
    "\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        self.size = netsize\t\t#Network size\n",
    "        self.Ssize = Nsensors  # Number of sensors\n",
    "        self.Msize = Nmotors  # Number of motors\n",
    "        self.Memsize = Nmemory # Number of memory units\n",
    "        self.Rewsize = Nreward # Number of units encoding reward\n",
    "        self.Inpsize = Nsensors + Nmemory + Nreward  # Number of sensors+memory+reward\n",
    "\n",
    "        self.h = np.zeros(netsize) # local biases\n",
    "        self.J = np.zeros((netsize, netsize)) # symmetic weights between hidden variables\n",
    "        self.max_weights = 2          # prevent (one of) the weights from growing too much\n",
    "\n",
    "        if predictor is not None:\n",
    "            self.predictor = predictor\n",
    "\n",
    "        self.env = gym.make('FrozenLake8x8-v0')\n",
    "        self.observation = self.env.reset()\n",
    "        self.maxobs = 64   # !!!!!! For frozen Lake\n",
    "        self.maxact = 4\n",
    "        self.maxrew = 3\n",
    "        \n",
    "        self.randomize_state() # randomise states s\n",
    "        self.initialise_sensors() # initialise sensors to point to start observation and to empty memory of the predictor\n",
    "\n",
    "        self.BetaCrit = 1.0  \n",
    "        self.defaultT = max(100, netsize * 20)\n",
    "\n",
    "        self.Ssize1 = 0\n",
    "        \n",
    "        self.rewardsPerEpisode = 0     #keep track of rewards (for crit)\n",
    "        self.successfulEpisodes = 0\n",
    "        \n",
    "    def get_state(self, mode='all'):\n",
    "        if mode == 'all':\n",
    "            return self.s\n",
    "        elif mode == 'motors':\n",
    "            return self.motors\n",
    "        elif mode == 'sensors': \n",
    "            return self.s[0:self.Inpsize]\n",
    "        elif mode == 'input':\n",
    "            return self.sensors\n",
    "        elif mode == 'non-sensors':\n",
    "            return self.s[self.Inpsize:]\n",
    "        elif mode == 'hidden':\n",
    "            return self.s[self.Inpsize:-self.Msize]\n",
    "\n",
    "    # gets the index of the configuration of the neurons - state as one number\n",
    "    def get_state_index(self, mode='all'):\n",
    "        return bool2int(0.5 * (self.get_state(mode) + 1))\n",
    "\n",
    "    def initialise_sensors(self):\n",
    "        self.sensors = np.repeat(-1, self.Ssize+self.Memsize+self.Rewsize)\n",
    "        self.sensors[:self.Ssize] = bitfield(self.InputToIndex(self.observation, self.maxobs, self.Ssize), self.Ssize) * 2 - 1\n",
    "    \n",
    "    # Randomize the state of the network\n",
    "    def randomize_state(self):     \n",
    "        self.s = np.random.randint(0, 2, self.size) * 2 - 1           # make units -1 or 1     \n",
    "    \n",
    "    # Randomize the position of the agent\n",
    "    def randomize_position(self):\n",
    "        self.observation = self.env.reset()\n",
    "\n",
    "    # Set random bias to sets of units of the system\n",
    "    # bias to hidden and action\n",
    "    def random_fields(self, max_weights=None):\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        self.h[self.Inpsize:] = max_weights * \\\n",
    "            (np.random.rand(self.size - self.Inpsize) * 2 - 1)\n",
    "\n",
    "    # Set random connections to sets of units of the system\n",
    "    def random_wiring(self, max_weights=None):  # Set random values for J\n",
    "        if max_weights is None:\n",
    "            max_weights = self.max_weights\n",
    "        for i in range(self.size):\n",
    "            for j in np.arange(i + 1, self.size):\n",
    "                if i < j and (i >= self.Inpsize or j >= self.Inpsize):  # don't add connections between the sensors\n",
    "                    # what about the motors? connection between the two will be added: doesn't matter because the correlations are not considered?\n",
    "                    self.J[i, j] = (np.random.rand(1) * 2 - 1) * self.max_weights\n",
    "        # get rid of connections between hidden units, and also motor (still dont know why this hasnt been done)\n",
    "        self.J[self.Inpsize:-self.Msize,self.Inpsize:-self.Msize] = 0  # between hidden\n",
    "        self.J[:self.Inpsize,-self.Msize:] = 0             # between sensor and motor\n",
    "        self.J[-self.Msize:,-self.Msize:] = 0            # between motor\n",
    "\n",
    "    # Update the position of the agent\n",
    "    def Move(self):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        # step in environment and collect state2 and ext_reward\n",
    "        action = int(np.digitize(np.sum(self.s[-self.Msize:]) / self.Msize, [-1.1, -1/2, 0, 1/2, 1.1])) - 1\n",
    "        state2, ext_reward, done, info = self.env.step(action)\n",
    "        \n",
    "        # update predictor's history and collect its memory\n",
    "        self.predictor.history[self.episode,self.t,:] = np.array([self.observation, action, state2, ext_reward])  # update predictor history\n",
    "        # m' = f(s, a)  \n",
    "        memory = self.predictor.get_states(np.array([self.observation, action]).reshape(1,-1), extended=False, continuation=True)\n",
    "        \n",
    "        # calculate internal reward\n",
    "        int_reward = np.tanh(self.predictor.calculateInternalReward())\n",
    "        if int_reward < 0:\n",
    "            int_reward = 0\n",
    "        reward = int_reward+ext_reward\n",
    "        \n",
    "        # new state = state2 + memory (from state,action) + reward (int + ext)\n",
    "        self.observation = state2  \n",
    "        self.UpdateSensors(state2,memory,reward) \n",
    "        \n",
    "#        self.rewardsPerEpisode += reward    #update rewards per episode\n",
    "#        self.observations[(self.observations == -1).argmax()] = observation      #add to list woth visited states\n",
    "    \n",
    "    # Transorm the sensor/motor input into integer index\n",
    "    def InputToIndex(self, x, xmax, bitsize):\n",
    "        return int(np.floor((x + xmax) / (2 * xmax + 10 * np.finfo(float).eps) * 2**bitsize))\n",
    "    \n",
    "    # Update the state of the sensor\n",
    "    def UpdateSensors(self, state, memory, reward):  \n",
    "        \n",
    "        # binarise observation\n",
    "        state_bit = bitfield(self.InputToIndex(state, self.maxobs, self.Ssize), self.Ssize) * 2 - 1 \n",
    "        self.sensors[:self.Ssize] = state_bit\n",
    "        # binarise memory\n",
    "        memory_bit = np.zeros((1, memory.shape[1]))\n",
    "        for i in range(memory.shape[1]):\n",
    "            memory_bit[0,i] = -1 if memory[0,i] < 0.5 else 1\n",
    "        self.sensors[self.Ssize:(self.Ssize+self.Memsize)]= memory_bit\n",
    "        # binarise reward\n",
    "        reward_bit = bitfield(self.InputToIndex(reward, self.maxrew, self.Rewsize), self.Rewsize) * 2 - 1\n",
    "        self.sensors[(self.Ssize+self.Memsize):self.Inpsize]= reward_bit\n",
    "        \n",
    "    # Updates only the observation of the sensor, the memory and reward are kept the same\n",
    "    def resetSensors(self, state):  \n",
    "        \n",
    "        state_bit = bitfield(self.InputToIndex(state, self.maxobs, self.Ssize), self.Ssize) * 2 - 1 \n",
    "        self.sensors[:self.Ssize] = state_bit\n",
    "\n",
    "    # Execute step of the Glauber algorithm to update the state of one unit\n",
    "    def GlauberStep(self, i=None): \n",
    "        if i is None:\n",
    "            i = np.random.randint(self.size)\n",
    "\n",
    "        I = 0\n",
    "        if i < self.Inpsize:\n",
    "            I = self.sensors[i]\n",
    "        eDiff = 2 * self.s[i] * (self.h[i] + I +\n",
    "                                 np.dot(self.J[i, :] + self.J[:, i], self.s))\n",
    "        if eDiff * self.BetaCrit < np.log(1 / np.random.rand() - 1):    # Glauber\n",
    "            self.s[i] = -self.s[i]\n",
    "\n",
    "    # Update random unit of the agent\n",
    "    def Update(self, i=None):\n",
    "        if i is None:\n",
    "            i = np.random.randint(-1, self.size)\n",
    "        if i == -1:\n",
    "            self.Move()\n",
    "        else:\n",
    "            self.GlauberStep(i)\n",
    "\n",
    "    # Sequentially update state of all units of the agent in random order\n",
    "    def SequentialUpdate(self):\n",
    "        for i in np.random.permutation(range(-1, self.size)):\n",
    "            self.Update(i)\n",
    "\n",
    "    # Step of the learning algorith to ajust correlations to the critical regime\n",
    "    def AdjustCorrelations(self, T=None):\n",
    "          \n",
    "        if T is None:\n",
    "            T = self.defaultT\n",
    "\n",
    "        self.m = np.zeros(self.size)\n",
    "        self.c = np.zeros((self.size, self.size))\n",
    "        self.C = np.zeros((self.size, self.size))\n",
    "\n",
    "        # Main simulation loop:\n",
    "        self.x = np.zeros(T)      # to store the positions of the car during all T\n",
    "        for t in range(T):\n",
    "            \n",
    "            self.SequentialUpdate()\n",
    "            self.x[t] = self.observation\n",
    "            self.m += self.s\n",
    "            for i in range(self.size):\n",
    "                self.c[i, i + 1:] += self.s[i] * self.s[i + 1:]\n",
    "            \n",
    "            self.t += 1\n",
    "            \n",
    "        self.m /= T\n",
    "        self.c /= T\n",
    "        for i in range(self.size):\n",
    "            self.C[i, i + 1:] = self.c[i, i + 1:] - self.m[i] * self.m[i + 1:]\n",
    "\n",
    "        c1 = np.zeros((self.size, self.size))\n",
    "        for i in range(self.size):\n",
    "            inds = np.array([], int)\n",
    "            c = np.array([])\n",
    "            for j in range(self.size):\n",
    "                if not i == j:\n",
    "                    inds = np.append(inds, [j])\n",
    "                if i < j:\n",
    "                    c = np.append(c, [self.c[i, j]])\n",
    "                elif i > j:\n",
    "                    c = np.append(c, [self.c[j, i]])\n",
    "            order = np.argsort(c)[::-1]\n",
    "            c1[i, inds[order]] = self.Cint[i, :]\n",
    "        self.c1 = np.triu(c1 + c1.T, 1)\n",
    "        self.c1 *= 0.5\n",
    "\n",
    "        self.m[0:self.Inpsize] = 0          \n",
    "        self.m1[0:self.Inpsize] = 0     #sensors have objective mean 0 but in the paper they say it's all of the units but the sensors that have mean 0??\n",
    "        self.c[0:self.Inpsize, 0:self.Inpsize] = 0    #set corr in between sensors to 0\n",
    "        self.c[-self.Msize:, -self.Msize:] = 0    #set corr in between motors to 0\n",
    "        self.c[0:self.Inpsize, -self.Msize:] = 0    #set corr between sensors and motors to 0\n",
    "        self.c1[0:self.Inpsize, 0:self.Inpsize] = 0\n",
    "        self.c1[-self.Msize:, -self.Msize:] = 0\n",
    "        self.c1[0:self.Inpsize, -self.Msize:] = 0\n",
    "        \n",
    "        # make it restricted BM\n",
    "        self.c[self.Inpsize:-self.Msize,self.Inpsize:-self.Msize] = 0   #set corr in between hidden units to 0\n",
    "        self.c1[self.Inpsize:-self.Msize,self.Inpsize:-self.Msize] = 0   #for reference as well\n",
    "        \n",
    "        dh = self.m1 - self.m\n",
    "        dJ = self.c1 - self.c\n",
    "\n",
    "        return dh, dJ\n",
    "\n",
    "    # Algorithm for poising an agent in a critical regime\n",
    "    def CriticalLearning(self, Iterations, T=None):\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        tic = time.perf_counter()\n",
    "    \n",
    "        self.observations = np.repeat(-1,Iterations*T)    #keep track of reached states\n",
    "        self.predictor.setHistory(int(Iterations/10)+1, T*10 ) # resetting the env every 10 iterations, so episodes and steps adjusted accordingly \n",
    "        \n",
    "        self.t = 0\n",
    "#         self.it = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        u = 0.01\n",
    "        count = 0\n",
    "        dh, dJ = self.AdjustCorrelations(T)\n",
    "        for it in range(Iterations):\n",
    "            \n",
    "#             print(self.episode, self.t)\n",
    "#             self.it += 1\n",
    "#             self.t += self.it*T\n",
    "            count += 1\n",
    "            self.h += u * dh\n",
    "            self.J += u * dJ\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                \n",
    "                toc = time.perf_counter()\n",
    "                print('Episode ' + str(self.episode) + ' took ' + str(int((toc - tic)/60)) + ' minutes.')\n",
    "                \n",
    "                tic = time.perf_counter()\n",
    "#                 pdb.set_trace()\n",
    "                \n",
    "                self.randomize_position()  # updates self.observation to be 0\n",
    "                \n",
    "                # update the sensors with observation=0, the memory and the reward are kept the same\n",
    "                self.resetSensors(self.observation)\n",
    "                self.randomize_state()\n",
    "                \n",
    "                self.episode += 1  # first episode will have T instead of T*10 entries but thats ok because the predictor's reward method only takes actual entries\n",
    "                self.t = 0\n",
    "#                 print('reset'+str(self.episode)+str(self.t))\n",
    "#                 self.it = 0\n",
    "                # !!!! different from original - it randomises the sensors as well.\n",
    "                \n",
    "#                if self.rewardsPerEpisode >= 1:     # keep track of the times the agent reached the goal\n",
    "#                    self.successfulEpisodes += 1\n",
    "#                self.rewardsPerEpisode = 0\n",
    "                \n",
    "            Vmax = self.max_weights\n",
    "            for i in range(self.size):\n",
    "                if np.abs(self.h[i]) > Vmax:        # why do we need to restrict the weights and biases to a maximum value?\n",
    "                    self.h[i] = Vmax * np.sign(self.h[i])\n",
    "                for j in np.arange(i + 1, self.size):\n",
    "                    if np.abs(self.J[i, j]) > Vmax:\n",
    "                        self.J[i, j] = Vmax * np.sign(self.J[i, j])\n",
    "\n",
    "            dh, dJ = self.AdjustCorrelations(T)\n",
    "\n",
    "        \n",
    "        \n",
    "# Transform bool array into positive integer\n",
    "# [nk] encodes the state of the neurons as one number\n",
    "def bool2int(x):\n",
    "    y = 0\n",
    "    for i, j in enumerate(np.array(x)[::-1]):\n",
    "        y += j * 2**i\n",
    "    return int(y)\n",
    "\n",
    "# Transform positive integer into bit array\n",
    "def bitfield(n, size):\n",
    "    x = [int(x) for x in bin(int(n))[2:]]\n",
    "    x = [0] * (size - len(x)) + x\n",
    "    return np.array(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 took 0 minutes.\n",
      "Episode 1 took 0 minutes.\n",
      "Episode 2 took 0 minutes.\n",
      "Episode 3 took 0 minutes.\n",
      "Episode 4 took 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# esn params\n",
    "predictor_inputs = 2 # linear state,action\n",
    "predictor_outputs = 2 # linear state2, reward\n",
    "predictor_reservoir = 12\n",
    "predictor_radius = 0.9\n",
    "predictor_sparsity = 0.9\n",
    "out_act = identity\n",
    "inv_out_act = identity\n",
    "\n",
    "predictor = ESN(n_inputs = predictor_inputs, n_outputs = predictor_outputs, n_reservoir=predictor_reservoir,\n",
    "                 spectral_radius=predictor_radius, sparsity=predictor_sparsity,\n",
    "                 out_activation=out_act, inverse_out_activation=inv_out_act,\n",
    "                input_bias = 1\n",
    "                 )\n",
    "\n",
    "# rbm params\n",
    "Ssize=7\n",
    "Msize=3\n",
    "Nhidden = 30 #more hidden due to more visible units: 23 sensors and 4 motors    #20 \n",
    "Nmemory = predictor_reservoir\n",
    "Nreward = 4\n",
    "size=Ssize+Nmemory+Nhidden+Nreward+Msize\n",
    "\n",
    "#actor\n",
    "I = ising(size,Nmemory,Nreward,Ssize,Msize,predictor)\n",
    "\n",
    "# Import reference correlations\n",
    "filecorr = 'correlations-ising2D-size400.npy'\n",
    "Cdist = np.load(filecorr)\n",
    "\n",
    "# reorder reference correlations to match network's current correlations\n",
    "I.m1 = np.zeros(size)\n",
    "I.Cint = np.zeros((size, size - 1))\n",
    "for i in range(size):\n",
    "    c = []\n",
    "    for j in range(size - 1):\n",
    "        ind = np.random.randint(len(Cdist))\n",
    "        c += [Cdist[ind]]\n",
    "    I.Cint[i, :] = -np.sort(-np.array(c))\n",
    "    \n",
    "# critical\n",
    "Iterations = 10000                          # originally 10000\n",
    "T = 1000                                    # originally 5000\n",
    "\n",
    "I.CriticalLearning(Iterations, T)\n",
    "\n",
    "# Params to save\n",
    "params = {}\n",
    "params['Ssize']= Ssize\n",
    "params['Msize']= Msize\n",
    "params['nhidden'] = Nhidden\n",
    "params['size'] = size\n",
    "params['Iterations'] = Iterations\n",
    "params['T'] = T\n",
    "params['network_J'] = I.J\n",
    "params['network_h'] = I.h\n",
    "\n",
    "# Name of the file to save run\n",
    "\n",
    "filetosave = 'critical-rbm+esn.pkl'\n",
    "\n",
    "# Save\n",
    "if not os.path.isfile(filetosave):\n",
    "    with open(filetosave, 'wb') as output:\n",
    "            pickle.dump(params, output, pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    print(\"File already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0,  0],\n",
       "        [ 0,  1,  0,  0],\n",
       "        [ 0,  2,  0,  0],\n",
       "        ...,\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1],\n",
       "        [-1, -1, -1, -1]],\n",
       "\n",
       "       [[ 0,  2,  0,  0],\n",
       "        [ 0,  1,  0,  0],\n",
       "        [ 0,  1,  0,  0],\n",
       "        ...,\n",
       "        [19,  1, 19,  0],\n",
       "        [19,  1, 19,  0],\n",
       "        [19,  0, 19,  0]],\n",
       "\n",
       "       [[ 0,  1,  1,  0],\n",
       "        [ 1,  0,  1,  0],\n",
       "        [ 1,  0,  1,  0],\n",
       "        ...,\n",
       "        [41,  1, 41,  0],\n",
       "        [41,  2, 41,  0],\n",
       "        [41,  0, 41,  0]],\n",
       "\n",
       "       [[ 0,  1,  0,  0],\n",
       "        [ 0,  1,  8,  0],\n",
       "        [ 8,  0,  8,  0],\n",
       "        ...,\n",
       "        [35,  0, 35,  0],\n",
       "        [35,  0, 35,  0],\n",
       "        [35,  2, 35,  0]],\n",
       "\n",
       "       [[ 0,  3,  0,  0],\n",
       "        [ 0,  2,  8,  0],\n",
       "        [ 8,  2,  0,  0],\n",
       "        ...,\n",
       "        [49,  3, 49,  0],\n",
       "        [49,  3, 49,  0],\n",
       "        [49,  3, 49,  0]],\n",
       "\n",
       "       [[ 0,  2,  8,  0],\n",
       "        [ 8,  0,  0,  0],\n",
       "        [ 0,  0,  8,  0],\n",
       "        ...,\n",
       "        [41,  3, 41,  0],\n",
       "        [41,  3, 41,  0],\n",
       "        [41,  3, 41,  0]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I.predictor.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
